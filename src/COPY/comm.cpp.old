/* ----------------------------------------------------------------------
   LAMMPS - Large-scale Atomic/Molecular Massively Parallel Simulator
   http://lammps.sandia.gov, Sandia National Laboratories
   Steve Plimpton, sjplimp@sandia.gov

   Copyright (2003) Sandia Corporation.  Under the terms of Contract
   DE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains
   certain rights in this software.  This software is distributed under
   the GNU General Public License.

   See the README file in the top-level LAMMPS directory.
------------------------------------------------------------------------- */

/* ----------------------------------------------------------------------
   Contributing author (triclinic) : Pieter in 't Veld (SNL)
------------------------------------------------------------------------- */

#include "lmptype.h"
#include "mpi.h"
#include "math.h"
#include <string>
#include "stdio.h"
#include "stdlib.h"
#include <fstream>
#include "comm.h"
#include "universe.h"
#include "atom.h"
#include "atom_vec.h"
#include "force.h"
#include "pair.h"
#include "domain.h"
#include "neighbor.h"
#include "group.h"
#include "modify.h"
#include "fix.h"
#include "compute.h"
#include "output.h"
#include "dump.h"
#include "procmap.h"
#include "math_extra.h"
#include "error.h"
#include "memory.h"
#include "update.h"

#ifdef _OPENMP
#include "omp.h"
#endif

using namespace std;
using namespace LAMMPS_NS;

#define BUFFACTOR 1.5
#define BUFMIN 1000
#define BUFEXTRA 1000
#define BIG 1.0e20
#define EPS 0.01
#define MAX_GBITS 64

enum{SINGLE,MULTI};
enum{MULTIPLE};                   // same as in ProcMap
enum{ONELEVEL,TWOLEVEL,NUMA,CUSTOM,USER};
enum{CART,CARTREORDER,XYZ};

/* ----------------------------------------------------------------------
   setup MPI and allocate buffer space
------------------------------------------------------------------------- */

Comm::Comm(LAMMPS *lmp) : Pointers(lmp)
{
  MPI_Comm_rank(world,&me);
  MPI_Comm_size(world,&nprocs);

  user_procgrid[0] = user_procgrid[1] = user_procgrid[2] = 0;
  coregrid[0] = coregrid[1] = coregrid[2] = 1;
  gridflag = ONELEVEL;
  mapflag = CART;
  customfile = NULL;
  recv_from_partition = send_to_partition = -1;
  otherflag = 0;
  outfile = NULL;

  grid2proc = NULL;

  bordergroup = 0;
  style = SINGLE;
  uniform = 1;
  xsplit = ysplit = zsplit = NULL;
  multilo = multihi = NULL;
  cutghostmulti = NULL;
  cutghostuser = 0.0;
  ghost_velocity = 0;

  user_part = processed_data_ind = 0;
  le = 0; u_le = 0.0; le_nall = 0; 
  shift = 0.0;
  le_nmax = 0; 
  le_sh = NULL;
  own_bin = NULL;
  nbinh = my_neigh = twice_comm = neigh_ind = NULL;
  m_comm = NULL;
  send_bit = exchange_bit = NULL;

  // use of OpenMP threads
  // query OpenMP for number of threads/process set by user at run-time
  // if the OMP_NUM_THREADS environment variable is not set, we default
  // to using 1 thread. This follows the principle of the least surprise,
  // while practically all OpenMP implementations violate it by using
  // as many threads as there are (virtual) CPU cores by default.

  nthreads = 1;
#ifdef _OPENMP
  if (getenv("OMP_NUM_THREADS") == NULL) {
    nthreads = 1;
    if (me == 0)
      error->warning(FLERR,"OMP_NUM_THREADS environment is not set.");
  } else {
    nthreads = omp_get_max_threads();
  }

  // enforce consistent number of threads across all MPI tasks

  MPI_Bcast(&nthreads,1,MPI_INT,0,world);
  omp_set_num_threads(nthreads);

  if (me == 0) {
    if (screen)
      fprintf(screen,"  using %d OpenMP thread(s) per MPI task\n",nthreads);
    if (logfile)
      fprintf(logfile,"  using %d OpenMP thread(s) per MPI task\n",nthreads);
  }
#endif

  // initialize comm buffers & exchange memory
  // NOTE: allow for AtomVec to set maxexchange_atom, e.g. for atom_style body
  
  maxexchange_atom = maxexchange_fix = 0;
  maxexchange = maxexchange_atom + maxexchange_fix;
  bufextra = maxexchange + BUFEXTRA;

  maxsend = BUFMIN;
  memory->create(buf_send,maxsend+bufextra,"comm:buf_send");
  maxrecv = BUFMIN;
  memory->create(buf_recv,maxrecv,"comm:buf_recv");

  maxswap = 6;
  allocate_swap(maxswap);

  sendlist = (int **) memory->smalloc(maxswap*sizeof(int *),"comm:sendlist");
  memory->create(maxsendlist,maxswap,"comm:maxsendlist");
  for (int i = 0; i < maxswap; i++) {
    maxsendlist[i] = BUFMIN;
    memory->create(sendlist[i],BUFMIN,"comm:sendlist[i]");
  }
}

/* ---------------------------------------------------------------------- */

Comm::~Comm()
{
  memory->destroy(xsplit);
  memory->destroy(ysplit);
  memory->destroy(zsplit);

  delete [] customfile;
  delete [] outfile;

  memory->destroy(grid2proc);

  free_swap();
  if (style == MULTI) {
    free_multi();
    memory->destroy(cutghostmulti);
  }

  if (sendlist) for (int i = 0; i < maxswap; i++) memory->destroy(sendlist[i]);
  memory->sfree(sendlist);
  memory->destroy(maxsendlist);

  memory->destroy(buf_send);
  memory->destroy(buf_recv);

  memory->sfree(le_sh);
  if (gridflag == USER){
    memory->destroy(own_bin);
    memory->destroy(nbinh);
    memory->destroy(my_neigh);
    memory->destroy(twice_comm);
    memory->destroy(neigh_ind);
    memory->destroy(m_comm);
    memory->destroy(send_bit);
    memory->destroy(exchange_bit);
  }
}

/* ----------------------------------------------------------------------
   create 3d grid of procs based on Nprocs and box size & shape
   map processors to grid, setup xyz split for a uniform grid
------------------------------------------------------------------------- */

void Comm::set_proc_grid(int outflag)
{
  // recv 3d proc grid of another partition if my 3d grid depends on it

  if (recv_from_partition >= 0) {
    MPI_Status status;
    if (me == 0) {
      MPI_Recv(other_procgrid,3,MPI_INT,
               universe->root_proc[recv_from_partition],0,
               universe->uworld,&status);
      MPI_Recv(other_coregrid,3,MPI_INT,
               universe->root_proc[recv_from_partition],0,
               universe->uworld,&status);
    }
    MPI_Bcast(other_procgrid,3,MPI_INT,0,world);
    MPI_Bcast(other_coregrid,3,MPI_INT,0,world);
  }

  // create ProcMap class to create 3d grid and map procs to it

  ProcMap *pmap = new ProcMap(lmp);

  // create 3d grid of processors
  // produces procgrid and coregrid (if relevant)

  if (gridflag == ONELEVEL) {
    pmap->onelevel_grid(nprocs,user_procgrid,procgrid,
                        otherflag,other_style,other_procgrid,other_coregrid);

  } else if (gridflag == TWOLEVEL) {
    pmap->twolevel_grid(nprocs,user_procgrid,procgrid,
                        ncores,user_coregrid,coregrid,
                        otherflag,other_style,other_procgrid,other_coregrid);

  } else if (gridflag == NUMA) {
    pmap->numa_grid(nprocs,user_procgrid,procgrid,coregrid);

  } else if (gridflag == CUSTOM) {
    pmap->custom_grid(customfile,nprocs,user_procgrid,procgrid);

  } else if (gridflag == USER) {
    read_user_file();
    for (int i = 0; i < 3; i++)
      myloc[i] = me;
    procgrid[0] = user_procgrid[0] =  nprocs;
    procgrid[1] = user_procgrid[1] = 1;
    procgrid[2] = user_procgrid[2] = 1; 
  }

  // error check on procgrid
  // should not be necessary due to ProcMap

  if (procgrid[0]*procgrid[1]*procgrid[2] != nprocs)
    error->all(FLERR,"Bad grid of processors");
  if (domain->dimension == 2 && procgrid[2] != 1)
    error->all(FLERR,"Processor count in z must be 1 for 2d simulation");

  // grid2proc[i][j][k] = proc that owns i,j,k location in 3d grid

  if (!user_part){
    if (grid2proc) memory->destroy(grid2proc);
    memory->create(grid2proc,procgrid[0],procgrid[1],procgrid[2],
                 "comm:grid2proc");
  }

  // map processor IDs to 3d processor grid
  // produces myloc, procneigh, grid2proc

  if (gridflag == ONELEVEL) {
    if (mapflag == CART)
      pmap->cart_map(0,procgrid,myloc,procneigh,grid2proc);
    else if (mapflag == CARTREORDER)
      pmap->cart_map(1,procgrid,myloc,procneigh,grid2proc);
    else if (mapflag == XYZ)
      pmap->xyz_map(xyz,procgrid,myloc,procneigh,grid2proc);

  } else if (gridflag == TWOLEVEL) {
    if (mapflag == CART)
      pmap->cart_map(0,procgrid,ncores,coregrid,myloc,procneigh,grid2proc);
    else if (mapflag == CARTREORDER)
      pmap->cart_map(1,procgrid,ncores,coregrid,myloc,procneigh,grid2proc);
    else if (mapflag == XYZ)
      pmap->xyz_map(xyz,procgrid,ncores,coregrid,myloc,procneigh,grid2proc);

  } else if (gridflag == NUMA) {
    pmap->numa_map(0,coregrid,myloc,procneigh,grid2proc);

  } else if (gridflag == CUSTOM) {
    pmap->custom_map(procgrid,myloc,procneigh,grid2proc);
  }

  // print 3d grid info to screen and logfile

  if (outflag && me == 0) {
    if (screen) {
      fprintf(screen,"  %d by %d by %d MPI processor grid\n",
              procgrid[0],procgrid[1],procgrid[2]);
      if (gridflag == NUMA || gridflag == TWOLEVEL)
        fprintf(screen,"  %d by %d by %d core grid within node\n",
                coregrid[0],coregrid[1],coregrid[2]);
    }
    if (logfile) {
      fprintf(logfile,"  %d by %d by %d MPI processor grid\n",
              procgrid[0],procgrid[1],procgrid[2]);
      if (gridflag == NUMA || gridflag == TWOLEVEL)
        fprintf(logfile,"  %d by %d by %d core grid within node\n",
                coregrid[0],coregrid[1],coregrid[2]);
    }
  }

  // print 3d grid details to outfile

  if (!user_part)
    if (outfile) pmap->output(outfile,procgrid,grid2proc);

  // free ProcMap class

  delete pmap;

  // set xsplit,ysplit,zsplit for uniform spacings

  memory->destroy(xsplit);
  memory->destroy(ysplit);
  memory->destroy(zsplit);

  memory->create(xsplit,procgrid[0]+1,"comm:xsplit");
  memory->create(ysplit,procgrid[1]+1,"comm:ysplit");
  memory->create(zsplit,procgrid[2]+1,"comm:zsplit");

  for (int i = 0; i < procgrid[0]; i++) xsplit[i] = i * 1.0/procgrid[0];
  for (int i = 0; i < procgrid[1]; i++) ysplit[i] = i * 1.0/procgrid[1];
  for (int i = 0; i < procgrid[2]; i++) zsplit[i] = i * 1.0/procgrid[2];

  xsplit[procgrid[0]] = ysplit[procgrid[1]] = zsplit[procgrid[2]] = 1.0;

  // set lamda box params after procs are assigned
  // only set once unless load-balancing occurs

  if (domain->triclinic) domain->set_lamda_box();

  // send my 3d proc grid to another partition if requested

  if (send_to_partition >= 0) {
    if (me == 0) {
      MPI_Send(procgrid,3,MPI_INT,
               universe->root_proc[send_to_partition],0,
               universe->uworld);
      MPI_Send(coregrid,3,MPI_INT,
               universe->root_proc[send_to_partition],0,
               universe->uworld);
    }
  }
}

/* ---------------------------------------------------------------------- */

void Comm::init()
{
  triclinic = domain->triclinic;
  map_style = atom->map_style;

  // comm_only = 1 if only x,f are exchanged in forward/reverse comm
  // comm_x_only = 0 if ghost_velocity since velocities are added

  comm_x_only = atom->avec->comm_x_only;
  comm_f_only = atom->avec->comm_f_only;
  if (ghost_velocity) comm_x_only = 0;

  // set per-atom sizes for forward/reverse/border comm
  // augment by velocity and fix quantities if needed

  size_forward = atom->avec->size_forward;
  size_reverse = atom->avec->size_reverse;
  size_border = atom->avec->size_border;

  if (ghost_velocity) size_forward += atom->avec->size_velocity;
  if (ghost_velocity) size_border += atom->avec->size_velocity;

  for (int i = 0; i < modify->nfix; i++)
    size_border += modify->fix[i]->comm_border;
  
  // maxexchange = max # of datums/atom in exchange communication
  // maxforward = # of datums in largest forward communication
  // maxreverse = # of datums in largest reverse communication
  // query pair,fix,compute,dump for their requirements
  // pair style can force reverse comm even if newton off

  maxexchange = BUFMIN + maxexchange_fix;
  maxforward = MAX(size_forward,size_border);
  maxreverse = size_reverse;

  if (force->pair) maxforward = MAX(maxforward,force->pair->comm_forward);
  if (force->pair) maxreverse = MAX(maxreverse,force->pair->comm_reverse);

  for (int i = 0; i < modify->nfix; i++) {
    maxforward = MAX(maxforward,modify->fix[i]->comm_forward);
    maxreverse = MAX(maxreverse,modify->fix[i]->comm_reverse);
  }

  for (int i = 0; i < modify->ncompute; i++) {
    maxforward = MAX(maxforward,modify->compute[i]->comm_forward);
    maxreverse = MAX(maxreverse,modify->compute[i]->comm_reverse);
  }

  for (int i = 0; i < output->ndump; i++) {
    maxforward = MAX(maxforward,output->dump[i]->comm_forward);
    maxreverse = MAX(maxreverse,output->dump[i]->comm_reverse);
  }

  if (force->newton == 0) maxreverse = 0;
  if (force->pair) maxreverse = MAX(maxreverse,force->pair->comm_reverse_off);

  // memory for multi-style communication

  if (style == MULTI && multilo == NULL) {
    allocate_multi(maxswap);
    memory->create(cutghostmulti,atom->ntypes+1,3,"comm:cutghostmulti");
  }
  if (style == SINGLE && multilo) {
    free_multi();
    memory->destroy(cutghostmulti);
  }
}

/* ----------------------------------------------------------------------
   setup spatial-decomposition communication patterns
   function of neighbor cutoff(s) & cutghostuser & current box size
   single style sets slab boundaries (slablo,slabhi) based on max cutoff
   multi style sets type-dependent slab boundaries (multilo,multihi)
------------------------------------------------------------------------- */

void Comm::setup()
{
  // cutghost[] = max distance at which ghost atoms need to be acquired
  // for orthogonal:
  //   cutghost is in box coords = neigh->cutghost in all 3 dims
  // for triclinic:
  //   neigh->cutghost = distance between tilted planes in box coords
  //   cutghost is in lamda coords = distance between those planes
  // for multi:
  //   cutghostmulti = same as cutghost, only for each atom type

  int i;
  int ntypes = atom->ntypes;
  double *prd,*sublo,*subhi;

  double cut = MAX(neighbor->cutneighmax,cutghostuser);
  periodicity = domain->periodicity;

  if (user_part){
    cutghost[0] = cutghost[1] = cutghost[2] = cut;
    if (processed_data_ind == 0){
      process_bin_data();
      build_comm_matrix();
    }
    //domain->set_local_box(); 
  } else {
    if (triclinic == 0) {
      prd = domain->prd;
      sublo = domain->sublo;
      subhi = domain->subhi;
      cutghost[0] = cutghost[1] = cutghost[2] = cut;

      if (style == MULTI) {
        double *cuttype = neighbor->cuttype;
        for (i = 1; i <= ntypes; i++)
          cutghostmulti[i][0] = cutghostmulti[i][1] = cutghostmulti[i][2] =
            cuttype[i];
      }

    } else {
      prd = domain->prd_lamda;
      sublo = domain->sublo_lamda;
      subhi = domain->subhi_lamda;
      double *h_inv = domain->h_inv;
      double length0,length1,length2;
      length0 = sqrt(h_inv[0]*h_inv[0] + h_inv[5]*h_inv[5] + h_inv[4]*h_inv[4]);
      cutghost[0] = cut * length0;
      length1 = sqrt(h_inv[1]*h_inv[1] + h_inv[3]*h_inv[3]);
      cutghost[1] = cut * length1;
      length2 = h_inv[2];
      cutghost[2] = cut * length2;

      if (style == MULTI) {
        double *cuttype = neighbor->cuttype;
        for (i = 1; i <= ntypes; i++) {
          cutghostmulti[i][0] = cuttype[i] * length0;
          cutghostmulti[i][1] = cuttype[i] * length1;
          cutghostmulti[i][2] = cuttype[i] * length2;
        }
      }
    }

    // recvneed[idim][0/1] = # of procs away I recv atoms from, within cutghost
    //   0 = from left, 1 = from right
    //   do not cross non-periodic boundaries, need[2] = 0 for 2d
    // sendneed[idim][0/1] = # of procs away I send atoms to
    //   0 = to left, 1 = to right
    //   set equal to recvneed[idim][1/0] of neighbor proc
    // maxneed[idim] = max procs away any proc recvs atoms in either direction
    // uniform = 1 = uniform sized sub-domains:
    //   maxneed is directly computable from sub-domain size
    //     limit to procgrid-1 for non-PBC
    //   recvneed = maxneed except for procs near non-PBC
    //   sendneed = recvneed of neighbor on each side
    // uniform = 0 = non-uniform sized sub-domains:
    //   compute recvneed via updown() which accounts for non-PBC
    //   sendneed = recvneed of neighbor on each side
    //   maxneed via Allreduce() of recvneed

    int left,right;

    if (uniform) {
      maxneed[0] = static_cast<int> (cutghost[0] * procgrid[0] / prd[0]) + 1;
      maxneed[1] = static_cast<int> (cutghost[1] * procgrid[1] / prd[1]) + 1;
      maxneed[2] = static_cast<int> (cutghost[2] * procgrid[2] / prd[2]) + 1;
      if (domain->dimension == 2) maxneed[2] = 0;
      if (!periodicity[0]) maxneed[0] = MIN(maxneed[0],procgrid[0]-1);
      if (!periodicity[1]) maxneed[1] = MIN(maxneed[1],procgrid[1]-1);
      if (!periodicity[2]) maxneed[2] = MIN(maxneed[2],procgrid[2]-1);

      if (!periodicity[0]) {
        recvneed[0][0] = MIN(maxneed[0],myloc[0]);
        recvneed[0][1] = MIN(maxneed[0],procgrid[0]-myloc[0]-1);
        left = myloc[0] - 1;
        if (left < 0) left = procgrid[0] - 1;
        sendneed[0][0] = MIN(maxneed[0],procgrid[0]-left-1);
        right = myloc[0] + 1;
        if (right == procgrid[0]) right = 0;
        sendneed[0][1] = MIN(maxneed[0],right);
      } else recvneed[0][0] = recvneed[0][1] =
               sendneed[0][0] = sendneed[0][1] = maxneed[0];

      if (!periodicity[1]) {
        recvneed[1][0] = MIN(maxneed[1],myloc[1]);
        recvneed[1][1] = MIN(maxneed[1],procgrid[1]-myloc[1]-1);
        left = myloc[1] - 1;
        if (left < 0) left = procgrid[1] - 1;
        sendneed[1][0] = MIN(maxneed[1],procgrid[1]-left-1);
        right = myloc[1] + 1;
        if (right == procgrid[1]) right = 0;
        sendneed[1][1] = MIN(maxneed[1],right);
      } else recvneed[1][0] = recvneed[1][1] =
               sendneed[1][0] = sendneed[1][1] = maxneed[1];

      if (!periodicity[2]) {
        recvneed[2][0] = MIN(maxneed[2],myloc[2]);
        recvneed[2][1] = MIN(maxneed[2],procgrid[2]-myloc[2]-1);
        left = myloc[2] - 1;
        if (left < 0) left = procgrid[2] - 1;
        sendneed[2][0] = MIN(maxneed[2],procgrid[2]-left-1);
        right = myloc[2] + 1;
        if (right == procgrid[2]) right = 0;
        sendneed[2][1] = MIN(maxneed[2],right);
      } else recvneed[2][0] = recvneed[2][1] =
               sendneed[2][0] = sendneed[2][1] = maxneed[2];

    } else {
      recvneed[0][0] = updown(0,0,myloc[0],prd[0],periodicity[0],xsplit);
      recvneed[0][1] = updown(0,1,myloc[0],prd[0],periodicity[0],xsplit);
      left = myloc[0] - 1;
      if (left < 0) left = procgrid[0] - 1;
      sendneed[0][0] = updown(0,1,left,prd[0],periodicity[0],xsplit);
      right = myloc[0] + 1;
      if (right == procgrid[0]) right = 0;
      sendneed[0][1] = updown(0,0,right,prd[0],periodicity[0],xsplit);

      recvneed[1][0] = updown(1,0,myloc[1],prd[1],periodicity[1],ysplit);
      recvneed[1][1] = updown(1,1,myloc[1],prd[1],periodicity[1],ysplit);
      left = myloc[1] - 1;
      if (left < 0) left = procgrid[1] - 1;
      sendneed[1][0] = updown(1,1,left,prd[1],periodicity[1],ysplit);
      right = myloc[1] + 1;
      if (right == procgrid[1]) right = 0;
      sendneed[1][1] = updown(1,0,right,prd[1],periodicity[1],ysplit);

      if (domain->dimension == 3) {
        recvneed[2][0] = updown(2,0,myloc[2],prd[2],periodicity[2],zsplit);
        recvneed[2][1] = updown(2,1,myloc[2],prd[2],periodicity[2],zsplit);
        left = myloc[2] - 1;
        if (left < 0) left = procgrid[2] - 1;
        sendneed[2][0] = updown(2,1,left,prd[2],periodicity[2],zsplit);
        right = myloc[2] + 1;
        if (right == procgrid[2]) right = 0;
        sendneed[2][1] = updown(2,0,right,prd[2],periodicity[2],zsplit);
      } else recvneed[2][0] = recvneed[2][1] =
               sendneed[2][0] = sendneed[2][1] = 0;

      int all[6];
      MPI_Allreduce(&recvneed[0][0],all,6,MPI_INT,MPI_MAX,world);
      maxneed[0] = MAX(all[0],all[1]);
      maxneed[1] = MAX(all[2],all[3]);
      maxneed[2] = MAX(all[4],all[5]);
    }

    // allocate comm memory

    nswap = 2 * (maxneed[0]+maxneed[1]+maxneed[2]);
    if (nswap > maxswap) grow_swap(nswap);

    // setup parameters for each exchange:
    // sendproc = proc to send to at each swap
    // recvproc = proc to recv from at each swap
    // for style SINGLE:
    //   slablo/slabhi = boundaries for slab of atoms to send at each swap
    //   use -BIG/midpt/BIG to insure all atoms included even if round-off occurs
    //   if round-off, atoms recvd across PBC can be < or > than subbox boundary
    //   note that borders() only loops over subset of atoms during each swap
    //   treat all as PBC here, non-PBC is handled in borders() via r/s need[][]
    // for style MULTI:
    //   multilo/multihi is same, with slablo/slabhi for each atom type
    // pbc_flag: 0 = nothing across a boundary, 1 = something across a boundary
    // pbc = -1/0/1 for PBC factor in each of 3/6 orthogonal/triclinic dirs
    // for triclinic, slablo/hi and pbc_border will be used in lamda (0-1) coords
    // 1st part of if statement is sending to the west/south/down
    // 2nd part of if statement is sending to the east/north/up

    int dim,ineed;

    int iswap = 0;
    for (dim = 0; dim < 3; dim++) {
      for (ineed = 0; ineed < 2*maxneed[dim]; ineed++) {
        pbc_flag[iswap] = 0;
        pbc[iswap][0] = pbc[iswap][1] = pbc[iswap][2] =
          pbc[iswap][3] = pbc[iswap][4] = pbc[iswap][5] = 0;

        if (ineed % 2 == 0) {
          sendproc[iswap] = procneigh[dim][0];
          recvproc[iswap] = procneigh[dim][1];
          if (style == SINGLE) {
            if (ineed < 2) slablo[iswap] = -BIG;
            else slablo[iswap] = 0.5 * (sublo[dim] + subhi[dim]);
            slabhi[iswap] = sublo[dim] + cutghost[dim];
          } else {
            for (i = 1; i <= ntypes; i++) {
              if (ineed < 2) multilo[iswap][i] = -BIG;
              else multilo[iswap][i] = 0.5 * (sublo[dim] + subhi[dim]);
              multihi[iswap][i] = sublo[dim] + cutghostmulti[i][dim];
            }
          }
          if (myloc[dim] == 0) {
            pbc_flag[iswap] = 1;
            pbc[iswap][dim] = 1;
            if (triclinic) {
              if (dim == 1) pbc[iswap][5] = 1;
              else if (dim == 2) pbc[iswap][4] = pbc[iswap][3] = 1;
            }
          }

        } else {
          sendproc[iswap] = procneigh[dim][1];
          recvproc[iswap] = procneigh[dim][0];
          if (style == SINGLE) {
            slablo[iswap] = subhi[dim] - cutghost[dim];
            if (ineed < 2) slabhi[iswap] = BIG;
            else slabhi[iswap] = 0.5 * (sublo[dim] + subhi[dim]);
          } else {
            for (i = 1; i <= ntypes; i++) {
              multilo[iswap][i] = subhi[dim] - cutghostmulti[i][dim];
              if (ineed < 2) multihi[iswap][i] = BIG;
              else multihi[iswap][i] = 0.5 * (sublo[dim] + subhi[dim]);
            }
          }
          if (myloc[dim] == procgrid[dim]-1) {
            pbc_flag[iswap] = 1;
            pbc[iswap][dim] = -1;
            if (triclinic) {
              if (dim == 1) pbc[iswap][5] = -1;
              else if (dim == 2) pbc[iswap][4] = pbc[iswap][3] = -1;
            }
          }
        }

        iswap++;
      }
    }
  }

}

/* ----------------------------------------------------------------------
   walk up/down the extent of nearby processors in dim and dir
   loc = myloc of proc to start at
   dir = 0/1 = walk to left/right
   do not cross non-periodic boundaries
   is not called for z dim in 2d
   return how many procs away are needed to encompass cutghost away from loc
------------------------------------------------------------------------- */

int Comm::updown(int dim, int dir, int loc,
                 double prd, int period_h, double *split)
{
  int index,count;
  double frac,delta;

  if (dir == 0) {
    frac = cutghost[dim]/prd;
    index = loc - 1;
    delta = 0.0;
    count = 0;
    while (delta < frac) {
      if (index < 0) {
        if (!period_h) break;
        index = procgrid[dim] - 1;
      }
      count++;
      delta += split[index+1] - split[index];
      index--;
    }

  } else {
    frac = cutghost[dim]/prd;
    index = loc + 1;
    delta = 0.0;
    count = 0;
    while (delta < frac) {
      if (index >= procgrid[dim]) {
        if (!period_h) break;
        index = 0;
      }
      count++;
      delta += split[index+1] - split[index];
      index++;
    }
  }

  return count;
}

/* ----------------------------------------------------------------------
   forward communication of atom coords every timestep
   other per-atom attributes may also be sent via pack/unpack routines
------------------------------------------------------------------------- */

void Comm::forward_comm(int dummy)
{
  int n,iswap,dim;
  MPI_Request request;
  MPI_Status status;
  AtomVec *avec = atom->avec;
  double **x = atom->x;
  double *buf;

  // exchange data with another proc
  // if other proc is self, just copy
  // if comm_x_only set, exchange or copy directly to x, don't unpack

  if (gridflag == USER){

    for (iswap = 0; iswap < n_neigh; iswap++) {
 
      if (comm_x_only) {
        if (size_forward_recv[iswap]) buf = x[firstrecv[iswap]];
        else buf = NULL;
        if (size_forward_recv[iswap])
          MPI_Irecv(buf,size_forward_recv[iswap],MPI_DOUBLE,
                    recvproc[iswap],recvproc[iswap],world,&request);
        n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);
        if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],me,world);
        if (size_forward_recv[iswap]) MPI_Wait(&request,&status);
      } else if (ghost_velocity) {
        if (size_forward_recv[iswap])
          MPI_Irecv(buf_recv,size_forward_recv[iswap],MPI_DOUBLE,
                    recvproc[iswap],recvproc[iswap],world,&request);
        n = avec->pack_comm_vel(sendnum[iswap],sendlist[iswap],
                                buf_send,pbc_flag[iswap],pbc[iswap]);
        if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],me,world);
        if (size_forward_recv[iswap]) MPI_Wait(&request,&status);
        avec->unpack_comm_vel(recvnum[iswap],firstrecv[iswap],buf_recv);
      } else {
        if (size_forward_recv[iswap])
          MPI_Irecv(buf_recv,size_forward_recv[iswap],MPI_DOUBLE,
                    recvproc[iswap],recvproc[iswap],world,&request);
        n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                            buf_send,pbc_flag[iswap],pbc[iswap]);
        if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],me,world);
        if (size_forward_recv[iswap]) MPI_Wait(&request,&status);
        avec->unpack_comm(recvnum[iswap],firstrecv[iswap],buf_recv);
      }
    }

    for (dim = 0; dim < n_own; dim++) {
      iswap = n_neigh + dim; 
      if (comm_x_only) {
        if (sendnum[iswap])
          n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                              x[firstrecv[iswap]],pbc_flag[iswap],
                              pbc[iswap]);
      } else if (ghost_velocity) {
        n = avec->pack_comm_vel(sendnum[iswap],sendlist[iswap],
                                buf_send,pbc_flag[iswap],pbc[iswap]);
        avec->unpack_comm_vel(recvnum[iswap],firstrecv[iswap],buf_send);
      } else {
        n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                            buf_send,pbc_flag[iswap],pbc[iswap]);
        avec->unpack_comm(recvnum[iswap],firstrecv[iswap],buf_send);
      }
    }
  
  } else {

    for (iswap = 0; iswap < nswap; iswap++) {

      // lees-edwards fix
      if (le && iswap == 2) lees_edwards(0);

      if (sendproc[iswap] != me) {
        if (comm_x_only) {
          if (size_forward_recv[iswap]) buf = x[firstrecv[iswap]];
          else buf = NULL;
          if (size_forward_recv[iswap])
            MPI_Irecv(buf,size_forward_recv[iswap],MPI_DOUBLE,
                      recvproc[iswap],0,world,&request);
          n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                            buf_send,pbc_flag[iswap],pbc[iswap]);
          if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],0,world);
          if (size_forward_recv[iswap]) MPI_Wait(&request,&status);
        } else if (ghost_velocity) {
          if (size_forward_recv[iswap])
            MPI_Irecv(buf_recv,size_forward_recv[iswap],MPI_DOUBLE,
                      recvproc[iswap],0,world,&request);
          n = avec->pack_comm_vel(sendnum[iswap],sendlist[iswap],
                                  buf_send,pbc_flag[iswap],pbc[iswap]);
          if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],0,world);
          if (size_forward_recv[iswap]) MPI_Wait(&request,&status);
          avec->unpack_comm_vel(recvnum[iswap],firstrecv[iswap],buf_recv);
        } else {
          if (size_forward_recv[iswap])
            MPI_Irecv(buf_recv,size_forward_recv[iswap],MPI_DOUBLE,
                      recvproc[iswap],0,world,&request);
          n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                              buf_send,pbc_flag[iswap],pbc[iswap]);
          if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],0,world);
          if (size_forward_recv[iswap]) MPI_Wait(&request,&status);
          avec->unpack_comm(recvnum[iswap],firstrecv[iswap],buf_recv);
        }

      } else {
        if (comm_x_only) {
          if (sendnum[iswap])
            n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                                x[firstrecv[iswap]],pbc_flag[iswap],
                                pbc[iswap]);
        } else if (ghost_velocity) {
          n = avec->pack_comm_vel(sendnum[iswap],sendlist[iswap],
                                  buf_send,pbc_flag[iswap],pbc[iswap]);
          avec->unpack_comm_vel(recvnum[iswap],firstrecv[iswap],buf_send);
        } else {
          n = avec->pack_comm(sendnum[iswap],sendlist[iswap],
                              buf_send,pbc_flag[iswap],pbc[iswap]);
          avec->unpack_comm(recvnum[iswap],firstrecv[iswap],buf_send);
        }
      }
    }

  }
}

/* ----------------------------------------------------------------------
   reverse communication of forces on atoms every timestep
   other per-atom attributes may also be sent via pack/unpack routines
------------------------------------------------------------------------- */

void Comm::reverse_comm()
{
  int n,iswap,dim;
  MPI_Request request;
  MPI_Status status;
  AtomVec *avec = atom->avec;
  double **f = atom->f;
  double *buf;

  // exchange data with another proc
  // if other proc is self, just copy
  // if comm_f_only set, exchange or copy directly from f, don't pack

  if (gridflag == USER){

    for (dim = n_own-1; dim >= 0; dim--) {
      iswap = n_neigh + dim; 
      if (comm_f_only) {
        if (sendnum[iswap])
          avec->unpack_reverse(sendnum[iswap],sendlist[iswap],
                              f[firstrecv[iswap]]);
      } else {
        n = avec->pack_reverse(recvnum[iswap],firstrecv[iswap],buf_send);
        avec->unpack_reverse(sendnum[iswap],sendlist[iswap],buf_send);
      }
    } 

    for (iswap = n_neigh-1; iswap >= 0; iswap--) {
      if (comm_f_only) {
        if (size_reverse_recv[iswap])
          MPI_Irecv(buf_recv,size_reverse_recv[iswap],MPI_DOUBLE,
                    sendproc[iswap],me,world,&request);
        if (size_reverse_send[iswap]) buf = f[firstrecv[iswap]];
        else buf = NULL;
        if (size_reverse_send[iswap])
          MPI_Send(buf,size_reverse_send[iswap],MPI_DOUBLE,
                   recvproc[iswap],recvproc[iswap],world);
        if (size_reverse_recv[iswap]) MPI_Wait(&request,&status);
      } else {
        if (size_reverse_recv[iswap])
          MPI_Irecv(buf_recv,size_reverse_recv[iswap],MPI_DOUBLE,
                    sendproc[iswap],me,world,&request);
        n = avec->pack_reverse(recvnum[iswap],firstrecv[iswap],buf_send);
        if (n) MPI_Send(buf_send,n,MPI_DOUBLE,recvproc[iswap],recvproc[iswap],world);
        if (size_reverse_recv[iswap]) MPI_Wait(&request,&status);
      }
      avec->unpack_reverse(sendnum[iswap],sendlist[iswap],buf_recv);
    }

  } else {

    for (iswap = nswap-1; iswap >= 0; iswap--) {
      if (sendproc[iswap] != me) {
        if (comm_f_only) {
          if (size_reverse_recv[iswap])
            MPI_Irecv(buf_recv,size_reverse_recv[iswap],MPI_DOUBLE,
                      sendproc[iswap],0,world,&request);
          if (size_reverse_send[iswap]) buf = f[firstrecv[iswap]];
          else buf = NULL;
          if (size_reverse_send[iswap])
            MPI_Send(buf,size_reverse_send[iswap],MPI_DOUBLE,
                     recvproc[iswap],0,world);
          if (size_reverse_recv[iswap]) MPI_Wait(&request,&status);
        } else {
          if (size_reverse_recv[iswap])
            MPI_Irecv(buf_recv,size_reverse_recv[iswap],MPI_DOUBLE,
                      sendproc[iswap],0,world,&request);
          n = avec->pack_reverse(recvnum[iswap],firstrecv[iswap],buf_send);
          if (n) MPI_Send(buf_send,n,MPI_DOUBLE,recvproc[iswap],0,world);
          if (size_reverse_recv[iswap]) MPI_Wait(&request,&status);
        }
        avec->unpack_reverse(sendnum[iswap],sendlist[iswap],buf_recv);

      } else {
        if (comm_f_only) {
          if (sendnum[iswap])
              avec->unpack_reverse(sendnum[iswap],sendlist[iswap],
                                  f[firstrecv[iswap]]);
        } else {
          n = avec->pack_reverse(recvnum[iswap],firstrecv[iswap],buf_send);
          avec->unpack_reverse(sendnum[iswap],sendlist[iswap],buf_send);
        }
      }
    }
  }
}

/* ----------------------------------------------------------------------
   exchange: move atoms to correct processors
   atoms exchanged with all 6 stencil neighbors
   send out atoms that have left my box, receive ones entering my box
   atoms will be lost if not inside some proc's box
     can happen if atom moves outside of non-periodic bounary
     or if atom moves more than one proc away
   this routine called before every reneighboring
   for triclinic, atoms must be in lamda coords (0-1) before exchange is called
------------------------------------------------------------------------- */

void Comm::exchange()
{
  int i,m,nsend,nrecv,nrecv1,nrecv2,nlocal,dim,nn[3],ind;
  double lo,hi,value;
  double **x;
  double *sublo,*subhi,*buf;
  MPI_Request request;
  MPI_Status status;
  AtomVec *avec = atom->avec;

  // clear global->local map for owned and ghost atoms
  // b/c atoms migrate to new procs in exchange() and
  //   new ghosts are created in borders()
  // map_set() is done at end of borders()
  // clear ghost count and any ghost bonus data internal to AtomVec

  if (map_style) atom->map_clear();
  atom->nghost = 0;
  atom->avec->clear_bonus();

  // insure send buf is large enough for single atom
  // fixes can change per-atom size requirement on-the-fly

  int bufextra_old = bufextra;
  maxexchange = maxexchange_atom + maxexchange_fix;
  bufextra = maxexchange + BUFEXTRA;
  if (bufextra > bufextra_old)
    memory->grow(buf_send,maxsend+bufextra,"comm:buf_send");

  // subbox bounds for orthogonal or triclinic

  if (triclinic == 0) {
    sublo = domain->sublo;
    subhi = domain->subhi;
  } else {
    sublo = domain->sublo_lamda;
    subhi = domain->subhi_lamda;
  }

  if (gridflag == USER){

    // loop over neighbors

    for (dim = 0; dim < n_neigh; dim++) {

      // fill buffer with atoms leaving my box, using comm matrix
      // when atom is deleted, fill it in with last atom

      x = atom->x;
      nlocal = atom->nlocal;
      i = nsend = 0;

      //if (me == 0) printf("dim=%d, matrix=%d \n",dim,m_comm[12][18][9]); 
      while (i < nlocal) {
        ind = coords_to_bin_exchange(x[i],nn); 
        //if (atom->tag[i] == 213 && (m_comm[nn[0]][nn[1]][nn[2]] & exchange_bit[dim]))
        //  printf("me=%d, step=%d, dim=%d, ind=%d, nn=%d %d %d, bit=%d, matrix=%d \n",me,update->ntimestep,dim,ind,nn[0],nn[1],nn[2],exchange_bit[dim],m_comm[nn[0]][nn[1]][nn[2]]); 
	//printf("hh me=%d, step=%d, dim=%d, nn=%d %d %d, ind=%d, nbp_loc=%d %d %d, x=%f %f %f, tag=%d, bit=%d, matrix=%d, mmat=%d \n",me,update->ntimestep,dim,nn[0],nn[1],nn[2],ind,nbp_loc[0],nbp_loc[1],nbp_loc[2],x[i][0],x[i][1],x[i][2],atom->tag[i],exchange_bit[dim],m_comm[nn[0]][nn[1]][nn[2]],m_comm[12][22][12]); 
        if (ind && (m_comm[nn[0]][nn[1]][nn[2]] & exchange_bit[dim])) {
          //if (nsend < 30)
	  //printf("hh me=%d, step=%d, dim=%d, nn=%d %d %d, ind=%d, nbp_loc=%d %d %d, x=%f %f %f, tag=%d, bit=%d, matrix=%d, mmat=%d \n",me,update->ntimestep,dim,nn[0],nn[1],nn[2],ind,nbp_loc[0],nbp_loc[1],nbp_loc[2],x[i][0],x[i][1],x[i][2],atom->tag[i],exchange_bit[dim],m_comm[nn[0]][nn[1]][nn[2]],m_comm[12][22][12]);
          if (nsend > maxsend) grow_send(nsend,1);
          nsend += avec->pack_exchange(i,&buf_send[nsend]);
          avec->copy(nlocal-1,i,1);
          nlocal--;
        } else 
          i++;
      }
      atom->nlocal = nlocal;

      // send/recv atoms in both directions
      //if (nsend > 0) printf("me=%d, step=%d, dim=%d, nsend=%d \n",me,update->ntimestep,dim,nsend);

      MPI_Sendrecv(&nsend,1,MPI_INT,sendproc[dim],me,
                   &nrecv1,1,MPI_INT,recvproc[dim],recvproc[dim],world,&status);
      nrecv = nrecv1;
      if (nrecv > maxrecv) grow_recv(nrecv);

      MPI_Irecv(buf_recv,nrecv1,MPI_DOUBLE,recvproc[dim],recvproc[dim],
                world,&request);
      MPI_Send(buf_send,nsend,MPI_DOUBLE,sendproc[dim],me,world);
      MPI_Wait(&request,&status);

      buf = buf_recv;

      // check incoming atoms to see if they are in my box
      // if so, add to my list

      m = 0;
      while (m < nrecv) {
        ind = coords_to_bin(&buf[m+1],nn);              // normally should not be checked, should remove after testing
        if ((ind == 0) && (m_comm[nn[0]][nn[1]][nn[2]] & local_bit))
          printf("00 me=%d, step=%d, dim=%d, nn=%d %d %d \n",me,update->ntimestep,dim,nn[0],nn[1],nn[2]); 
        if (ind && (m_comm[nn[0]][nn[1]][nn[2]] & local_bit)) {
          m += avec->unpack_exchange(&buf[m]);
        } else{ 
          //printf("11 me=%d, step=%d, dim=%d, nn=%d %d %d, ind=%d, nbp_loc=%d %d %d, x=%f %f %f, tag=%f, bit=%d, matrix=%d \n",me,update->ntimestep,dim,nn[0],nn[1],nn[2],ind,nbp_loc[0],nbp_loc[1],nbp_loc[2],buf[m+1],buf[m+2],buf[m+3],buf[m+7],local_bit,m_comm[nn[0]][nn[1]][nn[2]]); 
          m += static_cast<int> (buf[m]);
          error->one(FLERR,"Particles get lost in unpack exchange!");
        }
      }
    }
    
    x = atom->x; 
    for (i = 0; i < atom->nlocal; i++) {
      ind = coords_to_bin(x[i],nn);
      if (!(m_comm[nn[0]][nn[1]][nn[2]] & local_bit)){
        fprintf(stderr,"NOT A LOCAL PARTICLE - IT HAS BEEN DELETED: me=%d; step=%d; ind=%d; nn=%d %d %d; x=%f %f %f; tag=%d. \n",me,update->ntimestep,ind,nn[0],nn[1],nn[2],x[i][0],x[i][1],x[i][2],atom->tag[i]);
        avec->copy(atom->nlocal-1,i,1);
        atom->nlocal--;        
      } 
    }
    //sleep(5);

  } else {

    // loop over dimensions

    for (dim = 0; dim < 3; dim++) {

      // fill buffer with atoms leaving my box, using < and >=
      // when atom is deleted, fill it in with last atom

      x = atom->x;
      lo = sublo[dim];
      hi = subhi[dim];
      nlocal = atom->nlocal;
      i = nsend = 0;

      while (i < nlocal) {
        if (x[i][dim] < lo || x[i][dim] >= hi) {
          if (nsend > maxsend) grow_send(nsend,1);
          nsend += avec->pack_exchange(i,&buf_send[nsend]);
          avec->copy(nlocal-1,i,1);
          nlocal--;
        } else i++;
      }
      atom->nlocal = nlocal;

      // send/recv atoms in both directions
      // if 1 proc in dimension, no send/recv, set recv buf to send buf
      // if 2 procs in dimension, single send/recv
      // if more than 2 procs in dimension, send/recv to both neighbors

      if (procgrid[dim] == 1) {
        nrecv = nsend;
        buf = buf_send;

      } else {
        MPI_Sendrecv(&nsend,1,MPI_INT,procneigh[dim][0],0,
                     &nrecv1,1,MPI_INT,procneigh[dim][1],0,world,&status);
        nrecv = nrecv1;
        if (procgrid[dim] > 2) {
          MPI_Sendrecv(&nsend,1,MPI_INT,procneigh[dim][1],0,
                       &nrecv2,1,MPI_INT,procneigh[dim][0],0,world,&status);
          nrecv += nrecv2;
        }
        if (nrecv > maxrecv) grow_recv(nrecv);

        MPI_Irecv(buf_recv,nrecv1,MPI_DOUBLE,procneigh[dim][1],0,
                  world,&request);
        MPI_Send(buf_send,nsend,MPI_DOUBLE,procneigh[dim][0],0,world);
        MPI_Wait(&request,&status);

        if (procgrid[dim] > 2) {
          MPI_Irecv(&buf_recv[nrecv1],nrecv2,MPI_DOUBLE,procneigh[dim][0],0,
                    world,&request);
          MPI_Send(buf_send,nsend,MPI_DOUBLE,procneigh[dim][1],0,world);
          MPI_Wait(&request,&status);
        }

        buf = buf_recv;
      }

      // check incoming atoms to see if they are in my box
      // if so, add to my list

      m = 0;
      while (m < nrecv) {
        value = buf[m+dim+1];
        if (value >= lo && value < hi) m += avec->unpack_exchange(&buf[m]);
        else m += static_cast<int> (buf[m]);
      }
    }
  }

  if (atom->firstgroupname) atom->first_reorder();
}

/* ----------------------------------------------------------------------
   borders: list nearby atoms to send to neighboring procs at every timestep
   one list is created for every swap that will be made
   as list is made, actually do swaps
   this does equivalent of a communicate, so don't need to explicitly
     call communicate routine on reneighboring timestep
   this routine is called before every reneighboring
   for triclinic, atoms must be in lamda coords (0-1) before borders is called
------------------------------------------------------------------------- */

void Comm::borders()
{
  int i,j,n,itype,iswap,dim,ineed,twoneed,smax,rmax;
  int nsend,nrecv,sendflag,nfirst,nlast,ngroup,nn[3],ind;
  double lo,hi;
  int *type;
  double **x;
  double *buf,*mlo,*mhi;
  MPI_Request request;
  MPI_Status status;
  AtomVec *avec = atom->avec;

  // do swaps over all 3 dimensions

  iswap = 0;
  smax = rmax = 0;

  if (gridflag == USER){

    nlast = atom->nlocal;
    for (dim = 0; dim < n_neigh; dim++) {

      // find atoms within send bins 
      // check only local atoms 
      // store sent atom indices in list for use in future timesteps

      x = atom->x;
      nsend = 0;

      for (i = 0; i < nlast; i++){
        ind = coords_to_bin(x[i],nn);
        if (ind && (m_comm[nn[0]][nn[1]][nn[2]] & send_bit[dim])) {
          if (nsend == maxsendlist[dim]) grow_list(dim,nsend);
          sendlist[dim][nsend++] = i;
          //if (atom->tag[i] == 2177)
           // printf("SEND: me=%d; to=%d; x:%f %f %f; tag: %d; i=%d; nlocal=%d; pbc: %d - %d %d %d; nn: %d %d %d \n",me,sendproc[dim],x[i][0],x[i][1],x[i][2],atom->tag[i],i,atom->nlocal,pbc_flag[dim],pbc[dim][0],pbc[dim][1],pbc[dim][2],nn[0],nn[1],nn[2]);  
        }
      }

      // pack up list of border atoms   

      if (nsend*size_border > maxsend)
        grow_send(nsend*size_border,0);
      if (ghost_velocity)
        n = avec->pack_border_vel(nsend,sendlist[dim],buf_send,
                                  pbc_flag[dim],pbc[dim]);
      else
        n = avec->pack_border(nsend,sendlist[dim],buf_send,
                              pbc_flag[dim],pbc[dim]);

      // swap atoms with other proc
      // no MPI calls except SendRecv if nsend/nrecv = 0
      // put incoming ghosts at end of my atom arrays

      MPI_Sendrecv(&nsend,1,MPI_INT,sendproc[dim],me,
                   &nrecv,1,MPI_INT,recvproc[dim],recvproc[dim],world,&status);
      if (nrecv*size_border > maxrecv) grow_recv(nrecv*size_border);
      if (nrecv) MPI_Irecv(buf_recv,nrecv*size_border,MPI_DOUBLE,
                           recvproc[dim],recvproc[dim],world,&request);
      if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[dim],me,world);
      if (nrecv) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      if (ghost_velocity)
        avec->unpack_border_vel(nrecv,atom->nlocal+atom->nghost,buf);
      else
        avec->unpack_border(nrecv,atom->nlocal+atom->nghost,buf);

      // set all pointers & counters
 
      smax = MAX(smax,nsend);
      rmax = MAX(rmax,nrecv);
      sendnum[dim] = nsend;
      recvnum[dim] = nrecv;
      size_forward_recv[dim] = nrecv*size_forward;
      size_reverse_send[dim] = nrecv*size_reverse;
      size_reverse_recv[dim] = nsend*size_reverse;
      firstrecv[dim] = atom->nlocal + atom->nghost;
      atom->nghost += nrecv;
    }

    // copy ghost particles within the same core if needed
  
    for (dim = 0; dim < n_own; dim++) {
      j = n_neigh + dim;
      x = atom->x;
      nlast = atom->nlocal + atom->nghost;
      nsend = 0;

      for (i = 0; i < nlast; i++){
        ind = coords_to_bin(x[i],nn);
        if (ind && (m_comm[nn[0]][nn[1]][nn[2]] & own_bit[dim])) {
          if (nsend == maxsendlist[j]) grow_list(j,nsend);
          sendlist[j][nsend++] = i;
        }
      }

      if (nsend){ 

        // pack up list of border atoms

        if (nsend*size_border > maxsend) grow_send(nsend*size_border,0);
        if (ghost_velocity)
          n = avec->pack_border_vel(nsend,sendlist[j],buf_send,
                                pbc_flag[j],pbc[j]);
        else
          n = avec->pack_border(nsend,sendlist[j],buf_send,
                                pbc_flag[j],pbc[j]);         
   
        nrecv = nsend;
        buf = buf_send; 

        // unpack buffer

        if (ghost_velocity)
          avec->unpack_border_vel(nrecv,atom->nlocal+atom->nghost,buf);
        else
          avec->unpack_border(nrecv,atom->nlocal+atom->nghost,buf);

        // set all pointers & counters

        smax = MAX(smax,nsend);
        rmax = MAX(rmax,nrecv);
        sendnum[j] = nsend;
        recvnum[j] = nrecv;
        size_forward_recv[j] = nrecv*size_forward;
        size_reverse_send[j] = nrecv*size_reverse;
        size_reverse_recv[j] = nsend*size_reverse;
        firstrecv[j] = atom->nlocal + atom->nghost;
        atom->nghost += nrecv;
      }  
    }    

  } else {

    for (dim = 0; dim < 3; dim++) {
      nlast = 0;
      twoneed = 2*maxneed[dim];
      for (ineed = 0; ineed < twoneed; ineed++) {

        // find atoms within slab boundaries lo/hi using <= and >=
        // check atoms between nfirst and nlast
        //   for first swaps in a dim, check owned and ghost
        //   for later swaps in a dim, only check newly arrived ghosts
        // store sent atom indices in list for use in future timesteps

        x = atom->x;
        if (style == SINGLE) {
          lo = slablo[iswap];
          hi = slabhi[iswap];
        } else {
          type = atom->type;
          mlo = multilo[iswap];
          mhi = multihi[iswap];
        }
        if (ineed % 2 == 0) {
          nfirst = nlast;
          nlast = atom->nlocal + atom->nghost;
        }

        nsend = 0;

        // lees-edwards fix
        if (le && dim == 1 && ineed == 0) {
          le_nall = atom->nlocal + atom->nghost;
          lees_edwards(1);
        }   

        // sendflag = 0 if I do not send on this swap
        // sendneed test indicates receiver no longer requires data
        // e.g. due to non-PBC or non-uniform sub-domains

        if (ineed/2 >= sendneed[dim][ineed % 2]) sendflag = 0;
        else sendflag = 1;

        // find send atoms according to SINGLE vs MULTI
        // all atoms eligible versus atoms in bordergroup
        // only need to limit loop to bordergroup for first sends (ineed < 2)
        // on these sends, break loop in two: owned (in group) and ghost

        if (sendflag) {
          if (!bordergroup || ineed >= 2) {
            if (style == SINGLE) {
              for (i = nfirst; i < nlast; i++)
                if (x[i][dim] >= lo && x[i][dim] <= hi) {
                  if (nsend == maxsendlist[iswap]) grow_list(iswap,nsend);
                  sendlist[iswap][nsend++] = i;
                }
            } else {
              for (i = nfirst; i < nlast; i++) {
                itype = type[i];
                if (x[i][dim] >= mlo[itype] && x[i][dim] <= mhi[itype]) {
                  if (nsend == maxsendlist[iswap]) grow_list(iswap,nsend);
                  sendlist[iswap][nsend++] = i;
                }
              }
            }

          } else {
            if (style == SINGLE) {
              ngroup = atom->nfirst;
              for (i = 0; i < ngroup; i++)
                if (x[i][dim] >= lo && x[i][dim] <= hi) {
                  if (nsend == maxsendlist[iswap]) grow_list(iswap,nsend);
                  sendlist[iswap][nsend++] = i;
                }
              for (i = atom->nlocal; i < nlast; i++)
                if (x[i][dim] >= lo && x[i][dim] <= hi) {
                  if (nsend == maxsendlist[iswap]) grow_list(iswap,nsend);
                  sendlist[iswap][nsend++] = i;
                }
            } else {
              ngroup = atom->nfirst;
              for (i = 0; i < ngroup; i++) {
                itype = type[i];
                if (x[i][dim] >= mlo[itype] && x[i][dim] <= mhi[itype]) {
                  if (nsend == maxsendlist[iswap]) grow_list(iswap,nsend);
                  sendlist[iswap][nsend++] = i;
                }
              }
              for (i = atom->nlocal; i < nlast; i++) {
                itype = type[i];
                if (x[i][dim] >= mlo[itype] && x[i][dim] <= mhi[itype]) {
                  if (nsend == maxsendlist[iswap]) grow_list(iswap,nsend);
                  sendlist[iswap][nsend++] = i;
                }
              }
            }
          }
        }

        // pack up list of border atoms

        if (nsend*size_border > maxsend)
          grow_send(nsend*size_border,0);
        if (ghost_velocity)
          n = avec->pack_border_vel(nsend,sendlist[iswap],buf_send,
                                    pbc_flag[iswap],pbc[iswap]);
        else
          n = avec->pack_border(nsend,sendlist[iswap],buf_send,
                                pbc_flag[iswap],pbc[iswap]);

        // swap atoms with other proc
        // no MPI calls except SendRecv if nsend/nrecv = 0
        // put incoming ghosts at end of my atom arrays
        // if swapping with self, simply copy, no messages

        if (sendproc[iswap] != me) {
          MPI_Sendrecv(&nsend,1,MPI_INT,sendproc[iswap],0,
                       &nrecv,1,MPI_INT,recvproc[iswap],0,world,&status);
          if (nrecv*size_border > maxrecv) grow_recv(nrecv*size_border);
          if (nrecv) MPI_Irecv(buf_recv,nrecv*size_border,MPI_DOUBLE,
                               recvproc[iswap],0,world,&request);
          if (n) MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],0,world);
          if (nrecv) MPI_Wait(&request,&status);
          buf = buf_recv;
        } else {
          nrecv = nsend;
          buf = buf_send;
        }

        // unpack buffer

        if (ghost_velocity)
          avec->unpack_border_vel(nrecv,atom->nlocal+atom->nghost,buf);
        else
          avec->unpack_border(nrecv,atom->nlocal+atom->nghost,buf);

        // set all pointers & counters

        smax = MAX(smax,nsend);
        rmax = MAX(rmax,nrecv);
        sendnum[iswap] = nsend;
        recvnum[iswap] = nrecv;
        size_forward_recv[iswap] = nrecv*size_forward;
        size_reverse_send[iswap] = nrecv*size_reverse;
        size_reverse_recv[iswap] = nsend*size_reverse;
        firstrecv[iswap] = atom->nlocal + atom->nghost;
        atom->nghost += nrecv;
        iswap++;
      }
    }

  }

  // insure send/recv buffers are long enough for all forward & reverse comm

  int max = MAX(maxforward*smax,maxreverse*rmax);
  if (max > maxsend) grow_send(max,0);
  max = MAX(maxforward*rmax,maxreverse*smax);
  if (max > maxrecv) grow_recv(max);

  // reset global->local map

  if (map_style) atom->map_set();
}

/* ----------------------------------------------------------------------
   forward communication invoked by a Pair
------------------------------------------------------------------------- */

void Comm::forward_comm_pair(Pair *pair)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (iswap = 0; iswap < n_neigh; iswap++) {

      // pack buffer

      n = pair->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      
      if (recvnum[iswap])
        MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],
                  world,&request);
      if (sendnum[iswap])
        MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,world);
      if (recvnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      pair->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }

    for (dim = 0; dim < n_own; dim++) {
      iswap = n_neigh + dim;
      n = pair->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);
      buf = buf_send;
      pair->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);   
    }

  } else {

    for (iswap = 0; iswap < nswap; iswap++) {

      // pack buffer

      n = pair->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (recvnum[iswap])
          MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,
                    world,&request);
        if (sendnum[iswap])
          MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,world);
        if (recvnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      pair->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   reverse communication invoked by a Pair
------------------------------------------------------------------------- */

void Comm::reverse_comm_pair(Pair *pair)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){
    
    for (dim = n_own-1; dim >= 0; dim--) {
      iswap = n_neigh + dim;
      n = pair->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);
      buf = buf_send;
      pair->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    } 

    for (iswap = n_neigh-1; iswap >= 0; iswap--) {

      // pack buffer

      n = pair->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendnum[iswap])
        MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,
                    world,&request);
      if (recvnum[iswap])
        MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],world);
      if (sendnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      pair->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
  } else {

    for (iswap = nswap-1; iswap >= 0; iswap--) {

      // pack buffer

      n = pair->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
      if (sendnum[iswap])
      MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,
                  world,&request);
      if (recvnum[iswap])
        MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,world);
      if (sendnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;
    } else buf = buf_send;

      // unpack buffer

      pair->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   forward communication invoked by a Fix
   n = constant number of datums per atom
------------------------------------------------------------------------- */

void Comm::forward_comm_fix(Fix *fix)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (iswap = 0; iswap < n_neigh; iswap++) {

      // pack buffer

      n = fix->pack_comm(sendnum[iswap],sendlist[iswap],
                       buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (recvnum[iswap])
        MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],
                  world,&request);
      if (sendnum[iswap])
        MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,world);
      if (recvnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      fix->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }

    for (dim = 0; dim < n_own; dim++) {
      iswap = n_neigh + dim;
      n = fix->pack_comm(sendnum[iswap],sendlist[iswap],
                       buf_send,pbc_flag[iswap],pbc[iswap]);
      buf = buf_send;
      fix->unpack_comm(recvnum[iswap],firstrecv[iswap],buf); 
    }
  } else {

    for (iswap = 0; iswap < nswap; iswap++) {

      // pack buffer

      n = fix->pack_comm(sendnum[iswap],sendlist[iswap],
                       buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (recvnum[iswap])
          MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,
                    world,&request);
        if (sendnum[iswap])
          MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,world);
        if (recvnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      fix->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   reverse communication invoked by a Fix
   n = constant number of datums per atom
------------------------------------------------------------------------- */

void Comm::reverse_comm_fix(Fix *fix)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (dim = n_own-1; dim >= 0; dim--) {
      iswap = n_neigh + dim;
      n = fix->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);
      buf = buf_send;
      fix->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf); 
    }

    for (iswap = n_neigh-1; iswap >= 0; iswap--) {

      // pack buffer

      n = fix->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendnum[iswap])
        MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,
                  world,&request);
      if (recvnum[iswap])
        MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],world);
      if (sendnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      fix->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }

  } else {

    for (iswap = nswap-1; iswap >= 0; iswap--) {

      // pack buffer

      n = fix->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (sendnum[iswap])
          MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,
                    world,&request);
        if (recvnum[iswap])
          MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,world);
        if (sendnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      fix->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   forward communication invoked by a Fix
   n = total datums for all atoms, allows for variable number/atom
------------------------------------------------------------------------- */

void Comm::forward_comm_variable_fix(Fix *fix)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (iswap = 0; iswap < n_neigh; iswap++) {

      // pack buffer

      n = fix->pack_comm(sendnum[iswap],sendlist[iswap],
                       buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (recvnum[iswap])
        MPI_Irecv(buf_recv,maxrecv,MPI_DOUBLE,recvproc[iswap],recvproc[iswap],
                  world,&request);
      if (sendnum[iswap])
        MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],me,world);
      if (recvnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      fix->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }

    for (dim = 0; dim < n_own; dim++) {
      iswap = n_neigh + dim;
      n = fix->pack_comm(sendnum[iswap],sendlist[iswap],
                       buf_send,pbc_flag[iswap],pbc[iswap]);
      buf = buf_send;
      fix->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }
  } else {
  
    for (iswap = 0; iswap < nswap; iswap++) {
    
      // pack buffer
      
      n = fix->pack_comm(sendnum[iswap],sendlist[iswap],
                       buf_send,pbc_flag[iswap],pbc[iswap]);
                       
      // exchange with another proc
      // if self, set recv buffer to send buffer
      
      if (sendproc[iswap] != me) {
        if (recvnum[iswap])
          MPI_Irecv(buf_recv,maxrecv,MPI_DOUBLE,recvproc[iswap],0,
                    world,&request);
        if (sendnum[iswap])
          MPI_Send(buf_send,n,MPI_DOUBLE,sendproc[iswap],0,world);
        if (recvnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      fix->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   reverse communication invoked by a Fix
   n = total datums for all atoms, allows for variable number/atom
------------------------------------------------------------------------- */

void Comm::reverse_comm_variable_fix(Fix *fix)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (dim = n_own-1; dim >= 0; dim--) {
      iswap = n_neigh + dim;
      n = fix->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);
      buf = buf_send;
      fix->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
      
    for (iswap = n_neigh-1; iswap >= 0; iswap--) {
      
      // pack buffer
  
      n = fix->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendnum[iswap])
        MPI_Irecv(buf_recv,maxrecv,MPI_DOUBLE,sendproc[iswap],me,
                  world,&request);
      if (recvnum[iswap])
        MPI_Send(buf_send,n,MPI_DOUBLE,recvproc[iswap],recvproc[iswap],world);
      if (sendnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;
  
      // unpack buffer
  
      fix->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }

  } else {
  
    for (iswap = nswap-1; iswap >= 0; iswap--) {
    
      // pack buffer

      n = fix->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (sendnum[iswap])
          MPI_Irecv(buf_recv,maxrecv,MPI_DOUBLE,sendproc[iswap],0,
                    world,&request);
        if (recvnum[iswap])
          MPI_Send(buf_send,n,MPI_DOUBLE,recvproc[iswap],0,world);
        if (sendnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      fix->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   forward communication invoked by a Compute
------------------------------------------------------------------------- */

void Comm::forward_comm_compute(Compute *compute)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (iswap = 0; iswap < n_neigh; iswap++) {

      // pack buffer

      n = compute->pack_comm(sendnum[iswap],sendlist[iswap],
                             buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

    
      if (recvnum[iswap])
        MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],
                  world,&request);
      if (sendnum[iswap])
        MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,world);
      if (recvnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;
  

      // unpack buffer

      compute->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }

    for (dim = 0; dim < n_own; dim++) {
      iswap = n_neigh + dim;
      n = compute->pack_comm(sendnum[iswap],sendlist[iswap],
                             buf_send,pbc_flag[iswap],pbc[iswap]);
      buf = buf_send;
      compute->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }

  } else {

    for (iswap = 0; iswap < nswap; iswap++) {

      // pack buffer

      n = compute->pack_comm(sendnum[iswap],sendlist[iswap],
                             buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (recvnum[iswap])
          MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,
                    world,&request);
        if (sendnum[iswap])
          MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,world);
        if (recvnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      compute->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   reverse communication invoked by a Compute
------------------------------------------------------------------------- */

void Comm::reverse_comm_compute(Compute *compute)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (dim = n_own-1; dim >= 0; dim--) {
      iswap = n_neigh + dim;
      n = compute->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);
      buf = buf_send;
      compute->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf); 
    }

    for (iswap = n_neigh-1; iswap >= 0; iswap--) {

      // pack buffer

      n = compute->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendnum[iswap])
        MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,
                  world,&request);
      if (recvnum[iswap])
        MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],world);
      if (sendnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      compute->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }

  } else {

    for (iswap = nswap-1; iswap >= 0; iswap--) {

      // pack buffer

      n = compute->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (sendnum[iswap])
          MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,
                    world,&request);
        if (recvnum[iswap])
          MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,world);
        if (sendnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      compute->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   forward communication invoked by a Dump
------------------------------------------------------------------------- */

void Comm::forward_comm_dump(Dump *dump)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (iswap = 0; iswap < n_neigh; iswap++) {

      // pack buffer

      n = dump->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (recvnum[iswap])
        MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],
                  world,&request);
      if (sendnum[iswap])
        MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,world);
      if (recvnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      dump->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }

    for (dim = 0; dim < n_own; dim++) {
      iswap = n_neigh + dim;
      n = dump->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);
      buf = buf_send;
      dump->unpack_comm(recvnum[iswap],firstrecv[iswap],buf); 
    }

  } else {

    for (iswap = 0; iswap < nswap; iswap++) {

      // pack buffer

      n = dump->pack_comm(sendnum[iswap],sendlist[iswap],
                          buf_send,pbc_flag[iswap],pbc[iswap]);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (recvnum[iswap])
          MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,
                    world,&request);
        if (sendnum[iswap])
          MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,world);
        if (recvnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      dump->unpack_comm(recvnum[iswap],firstrecv[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   reverse communication invoked by a Dump
------------------------------------------------------------------------- */

void Comm::reverse_comm_dump(Dump *dump)
{
  int iswap,n,dim;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  if (gridflag == USER){

    for (dim = n_own-1; dim >= 0; dim--) {
      iswap = n_neigh + dim;
      n = dump->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);
      buf = buf_send;
      dump->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }

    for (iswap = n_neigh-1; iswap >= 0; iswap--) {

      // pack buffer

      n = dump->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendnum[iswap])
        MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],me,
                  world,&request);
      if (recvnum[iswap])
        MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],recvproc[iswap],world);
      if (sendnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;

      // unpack buffer

      dump->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }

  } else {

    for (iswap = nswap-1; iswap >= 0; iswap--) {

      // pack buffer

      n = dump->pack_reverse_comm(recvnum[iswap],firstrecv[iswap],buf_send);

      // exchange with another proc
      // if self, set recv buffer to send buffer

      if (sendproc[iswap] != me) {
        if (sendnum[iswap])
          MPI_Irecv(buf_recv,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,
                    world,&request);
        if (recvnum[iswap])
          MPI_Send(buf_send,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,world);
        if (sendnum[iswap]) MPI_Wait(&request,&status);
        buf = buf_recv;
      } else buf = buf_send;

      // unpack buffer

      dump->unpack_reverse_comm(sendnum[iswap],sendlist[iswap],buf);
    }
  }
}

/* ----------------------------------------------------------------------
   forward communication of N values in array
------------------------------------------------------------------------- */

void Comm::forward_comm_array(int n, double **array)
{
  int i,j,k,m,iswap,last;
  double *buf;
  MPI_Request request;
  MPI_Status status;

  // NOTE: check that buf_send and buf_recv are big enough

  for (iswap = 0; iswap < nswap; iswap++) {

    // pack buffer

    m = 0;
    for (i = 0; i < sendnum[iswap]; i++) {
      j = sendlist[iswap][i];
      for (k = 0; k < n; k++)
        buf_send[m++] = array[j][k];
    }

    // exchange with another proc
    // if self, set recv buffer to send buffer

    if (sendproc[iswap] != me) {
      if (recvnum[iswap])
        MPI_Irecv(buf_recv,n*recvnum[iswap],MPI_DOUBLE,recvproc[iswap],0,
                  world,&request);
      if (sendnum[iswap])
        MPI_Send(buf_send,n*sendnum[iswap],MPI_DOUBLE,sendproc[iswap],0,world);
      if (recvnum[iswap]) MPI_Wait(&request,&status);
      buf = buf_recv;
    } else buf = buf_send;

    // unpack buffer

    m = 0;
    last = firstrecv[iswap] + recvnum[iswap];
    for (i = firstrecv[iswap]; i < last; i++)
      for (k = 0; k < n; k++)
        array[i][k] = buf[m++];
  }
}

/* ----------------------------------------------------------------------
   exchange info provided with all 6 stencil neighbors
------------------------------------------------------------------------- */

int Comm::exchange_variable(int n, double *inbuf, double *&outbuf)
{
  int nsend,nrecv,nrecv1,nrecv2;
  MPI_Request request;
  MPI_Status status;

  nrecv = n;
  if (nrecv > maxrecv) grow_recv(nrecv);
  memcpy(buf_recv,inbuf,nrecv*sizeof(double)); 

  // loop over dimensions
  
  for (int dim = 0; dim < 3; dim++) {
    
    // no exchange if only one proc in a dimension
  
    if (procgrid[dim] == 1) continue;
  
    // send/recv info in both directions using same buf_recv
    // if 2 procs in dimension, single send/recv
    // if more than 2 procs in dimension, send/recv to both neighbors

    nsend = nrecv;
    MPI_Sendrecv(&nsend,1,MPI_INT,procneigh[dim][0],0,
                 &nrecv1,1,MPI_INT,procneigh[dim][1],0,world,&status);
    nrecv += nrecv1;
    if (procgrid[dim] > 2) {
      MPI_Sendrecv(&nsend,1,MPI_INT,procneigh[dim][1],0,
                   &nrecv2,1,MPI_INT,procneigh[dim][0],0,world,&status);
      nrecv += nrecv2;
    } else nrecv2 = 0;

    if (nrecv > maxrecv) grow_recv(nrecv);

    MPI_Irecv(&buf_recv[nsend],nrecv1,MPI_DOUBLE,procneigh[dim][1],0,
              world,&request);
    MPI_Send(buf_recv,nsend,MPI_DOUBLE,procneigh[dim][0],0,world);
    MPI_Wait(&request,&status);

    if (procgrid[dim] > 2) {
      MPI_Irecv(&buf_recv[nsend+nrecv1],nrecv2,MPI_DOUBLE,procneigh[dim][0],0,
                world,&request);
      MPI_Send(buf_recv,nsend,MPI_DOUBLE,procneigh[dim][1],0,world);
      MPI_Wait(&request,&status);
    }
  }

  outbuf = buf_recv;
  return nrecv;
}

/* ----------------------------------------------------------------------
   communicate inbuf around full ring of processors with messtag
   nbytes = size of inbuf = n datums * nper bytes
   callback() is invoked to allow caller to process/update each proc's inbuf
   if self=1 (default), then callback() is invoked on final iteration
     using original inbuf, which may have been updated
   for non-NULL outbuf, final updated inbuf is copied to it
     outbuf = inbuf is OK
------------------------------------------------------------------------- */

void Comm::ring(int n, int nper, void *inbuf, int messtag,
                void (*callback)(int, char *), void *outbuf, int self)
{
  MPI_Request request;
  MPI_Status status;

  int nbytes = n*nper;
  int maxbytes;
  MPI_Allreduce(&nbytes,&maxbytes,1,MPI_INT,MPI_MAX,world);

  char *buf,*bufcopy;
  memory->create(buf,maxbytes,"comm:buf");
  memory->create(bufcopy,maxbytes,"comm:bufcopy");
  memcpy(buf,inbuf,nbytes);

  int next = me + 1;
  int prev = me - 1;
  if (next == nprocs) next = 0;
  if (prev < 0) prev = nprocs - 1;

  for (int loop = 0; loop < nprocs; loop++) {
    if (me != next) {
      MPI_Irecv(bufcopy,maxbytes,MPI_CHAR,prev,messtag,world,&request);
      MPI_Send(buf,nbytes,MPI_CHAR,next,messtag,world);
      MPI_Wait(&request,&status);
      MPI_Get_count(&status,MPI_CHAR,&nbytes);
      memcpy(buf,bufcopy,nbytes);
    }
    if (self || loop < nprocs-1) callback(nbytes/nper,buf);
  }

  if (outbuf) memcpy(outbuf,buf,nbytes);

  memory->destroy(buf);
  memory->destroy(bufcopy);
}

/* ----------------------------------------------------------------------
   proc 0 reads Nlines from file into buf and bcasts buf to all procs
   caller allocates buf to max size needed
   each line is terminated by newline, even if last line in file is not
   return 0 if successful, 1 if get EOF error before read is complete
------------------------------------------------------------------------- */

int Comm::read_lines_from_file(FILE *fp, int nlines, int maxline, char *buf)
{
  int m;

  if (me == 0) {
    m = 0;
    for (int i = 0; i < nlines; i++) {
      if (!fgets(&buf[m],maxline,fp)) {
        m = 0;
        break;
      }
      m += strlen(&buf[m]);
    }
    if (m) {
      if (buf[m-1] != '\n') strcpy(&buf[m++],"\n");
      m++;
    }
  }
  
  MPI_Bcast(&m,1,MPI_INT,0,world);
  if (m == 0) return 1;
  MPI_Bcast(buf,m,MPI_CHAR,0,world);
  return 0;
} 
  
/* ----------------------------------------------------------------------
   proc 0 reads Nlines from file into buf and bcasts buf to all procs
   caller allocates buf to max size needed
   each line is terminated by newline, even if last line in file is not
   return 0 if successful, 1 if get EOF error before read is complete
------------------------------------------------------------------------- */

int Comm::read_lines_from_file_universe(FILE *fp, int nlines, int maxline,
                                        char *buf)
{
  int m;

  int me_universe = universe->me;
  MPI_Comm uworld = universe->uworld;

  if (me_universe == 0) {
    m = 0;
    for (int i = 0; i < nlines; i++) {
      if (!fgets(&buf[m],maxline,fp)) {
        m = 0;
        break;
      }
      m += strlen(&buf[m]);
    }
    if (m) {
      if (buf[m-1] != '\n') strcpy(&buf[m++],"\n");
      m++;
    }
  }

  MPI_Bcast(&m,1,MPI_INT,0,uworld);
  if (m == 0) return 1;
  MPI_Bcast(buf,m,MPI_CHAR,0,uworld);
  return 0;
}

/* ----------------------------------------------------------------------
   realloc the size of the send buffer as needed with BUFFACTOR and bufextra
   if flag = 1, realloc
   if flag = 0, don't need to realloc with copy, just free/malloc
------------------------------------------------------------------------- */

void Comm::grow_send(int n, int flag)
{
  maxsend = static_cast<int> (BUFFACTOR * n);
  if (flag)
    memory->grow(buf_send,maxsend+bufextra,"comm:buf_send");
  else {
    memory->destroy(buf_send);
    memory->create(buf_send,maxsend+bufextra,"comm:buf_send");
  }
}

/* ----------------------------------------------------------------------
   free/malloc the size of the recv buffer as needed with BUFFACTOR
------------------------------------------------------------------------- */

void Comm::grow_recv(int n)
{
  maxrecv = static_cast<int> (BUFFACTOR * n);
  memory->destroy(buf_recv);
  memory->create(buf_recv,maxrecv,"comm:buf_recv");
}

/* ----------------------------------------------------------------------
   realloc the size of the iswap sendlist as needed with BUFFACTOR
------------------------------------------------------------------------- */

void Comm::grow_list(int iswap, int n)
{
  maxsendlist[iswap] = static_cast<int> (BUFFACTOR * n);
  memory->grow(sendlist[iswap],maxsendlist[iswap],"comm:sendlist[iswap]");
}

/* ----------------------------------------------------------------------
   realloc the buffers needed for swaps
------------------------------------------------------------------------- */

void Comm::grow_swap(int n)
{
  free_swap();
  allocate_swap(n);
  if (style == MULTI) {
    free_multi();
    allocate_multi(n);
  }

  sendlist = (int **)
    memory->srealloc(sendlist,n*sizeof(int *),"comm:sendlist");
  memory->grow(maxsendlist,n,"comm:maxsendlist");
  for (int i = maxswap; i < n; i++) {
    maxsendlist[i] = BUFMIN;
    memory->create(sendlist[i],BUFMIN,"comm:sendlist[i]");
  }
  maxswap = n;
}

/* ----------------------------------------------------------------------
   allocation of swap info
------------------------------------------------------------------------- */

void Comm::allocate_swap(int n)
{
  memory->create(sendnum,n,"comm:sendnum");
  memory->create(recvnum,n,"comm:recvnum");
  memory->create(sendproc,n,"comm:sendproc");
  memory->create(recvproc,n,"comm:recvproc");
  memory->create(size_forward_recv,n,"comm:size");
  memory->create(size_reverse_send,n,"comm:size");
  memory->create(size_reverse_recv,n,"comm:size");
  memory->create(slablo,n,"comm:slablo");
  memory->create(slabhi,n,"comm:slabhi");
  memory->create(firstrecv,n,"comm:firstrecv");
  memory->create(pbc_flag,n,"comm:pbc_flag");
  memory->create(pbc,n,6,"comm:pbc");
}

/* ----------------------------------------------------------------------
   allocation of multi-type swap info
------------------------------------------------------------------------- */

void Comm::allocate_multi(int n)
{
  multilo = memory->create(multilo,n,atom->ntypes+1,"comm:multilo");
  multihi = memory->create(multihi,n,atom->ntypes+1,"comm:multihi");
}

/* ----------------------------------------------------------------------
   free memory for swaps
------------------------------------------------------------------------- */

void Comm::free_swap()
{
  memory->destroy(sendnum);
  memory->destroy(recvnum);
  memory->destroy(sendproc);
  memory->destroy(recvproc);
  memory->destroy(size_forward_recv);
  memory->destroy(size_reverse_send);
  memory->destroy(size_reverse_recv);
  memory->destroy(slablo);
  memory->destroy(slabhi);
  memory->destroy(firstrecv);
  memory->destroy(pbc_flag);
  memory->destroy(pbc);
}

/* ----------------------------------------------------------------------
   free memory for multi-type swaps
------------------------------------------------------------------------- */

void Comm::free_multi()
{
  memory->destroy(multilo);
  memory->destroy(multihi);
}

/* ----------------------------------------------------------------------
   set communication style
   invoked from input script by communicate command
------------------------------------------------------------------------- */

void Comm::set(int narg, char **arg)
{
  if (narg < 1) error->all(FLERR,"Illegal communicate command");

  if (strcmp(arg[0],"single") == 0) style = SINGLE;
  else if (strcmp(arg[0],"multi") == 0) style = MULTI;
  else error->all(FLERR,"Illegal communicate command");

  int iarg = 1;
  while (iarg < narg) {
    if (strcmp(arg[iarg],"group") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal communicate command");
      bordergroup = group->find(arg[iarg+1]);
      if (bordergroup < 0)
        error->all(FLERR,"Invalid group in communicate command");
      if (bordergroup && (atom->firstgroupname == NULL ||
                          strcmp(arg[iarg+1],atom->firstgroupname) != 0))
        error->all(FLERR,"Communicate group != atom_modify first group");
      iarg += 2;
    } else if (strcmp(arg[iarg],"cutoff") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal communicate command");
      cutghostuser = force->numeric(FLERR,arg[iarg+1]);
      if (cutghostuser < 0.0)
        error->all(FLERR,"Invalid cutoff in communicate command");
      iarg += 2;
    } else if (strcmp(arg[iarg],"vel") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal communicate command");
      if (strcmp(arg[iarg+1],"yes") == 0) ghost_velocity = 1;
      else if (strcmp(arg[iarg+1],"no") == 0) ghost_velocity = 0;
      else error->all(FLERR,"Illegal communicate command");
      iarg += 2;
    } else error->all(FLERR,"Illegal communicate command");
  }
}

/* ----------------------------------------------------------------------
   set dimensions for 3d grid of processors, and associated flags
   invoked from input script by processors command
------------------------------------------------------------------------- */

void Comm::set_processors(int narg, char **arg)
{
  if (narg < 3) error->all(FLERR,"Illegal processors command");

  if (strcmp(arg[0],"*") == 0) user_procgrid[0] = 0;
  else user_procgrid[0] = force->inumeric(FLERR,arg[0]);
  if (strcmp(arg[1],"*") == 0) user_procgrid[1] = 0;
  else user_procgrid[1] = force->inumeric(FLERR,arg[1]);
  if (strcmp(arg[2],"*") == 0) user_procgrid[2] = 0;
  else user_procgrid[2] = force->inumeric(FLERR,arg[2]);

  if (user_procgrid[0] < 0 || user_procgrid[1] < 0 || user_procgrid[2] < 0)
    error->all(FLERR,"Illegal processors command");

  int p = user_procgrid[0]*user_procgrid[1]*user_procgrid[2];
  if (p && p != nprocs)
    error->all(FLERR,"Specified processors != physical processors");

  int iarg = 3;
  while (iarg < narg) {
    if (strcmp(arg[iarg],"grid") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal processors command");

      if (strcmp(arg[iarg+1],"onelevel") == 0) {
        gridflag = ONELEVEL;

      } else if (strcmp(arg[iarg+1],"twolevel") == 0) {
        if (iarg+6 > narg) error->all(FLERR,"Illegal processors command");
        gridflag = TWOLEVEL;

        ncores = force->inumeric(FLERR,arg[iarg+2]);
        if (strcmp(arg[iarg+3],"*") == 0) user_coregrid[0] = 0;
        else user_coregrid[0] = force->inumeric(FLERR,arg[iarg+3]);
        if (strcmp(arg[iarg+4],"*") == 0) user_coregrid[1] = 0;
        else user_coregrid[1] = force->inumeric(FLERR,arg[iarg+4]);
        if (strcmp(arg[iarg+5],"*") == 0) user_coregrid[2] = 0;
        else user_coregrid[2] = force->inumeric(FLERR,arg[iarg+5]);

        if (ncores <= 0 || user_coregrid[0] < 0 ||
            user_coregrid[1] < 0 || user_coregrid[2] < 0)
          error->all(FLERR,"Illegal processors command");
        iarg += 4;

      } else if (strcmp(arg[iarg+1],"numa") == 0) {
        gridflag = NUMA;

      } else if (strcmp(arg[iarg],"custom") == 0) {
        if (iarg+3 > narg) error->all(FLERR,"Illegal processors command");
        gridflag = CUSTOM;
        delete [] customfile;
        int n = strlen(arg[iarg+2]) + 1;
        customfile = new char[n];
        strcpy(customfile,arg[iarg+2]);
        iarg += 1;

      } else error->all(FLERR,"Illegal processors command");
      iarg += 2;

    } else if (strcmp(arg[iarg],"map") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal processors command");
      if (strcmp(arg[iarg+1],"cart") == 0) mapflag = CART;
      else if (strcmp(arg[iarg+1],"cart/reorder") == 0) mapflag = CARTREORDER;
      else if (strcmp(arg[iarg+1],"xyz") == 0 ||
               strcmp(arg[iarg+1],"xzy") == 0 ||
               strcmp(arg[iarg+1],"yxz") == 0 ||
               strcmp(arg[iarg+1],"yzx") == 0 ||
               strcmp(arg[iarg+1],"zxy") == 0 ||
               strcmp(arg[iarg+1],"zyx") == 0) {
        mapflag = XYZ;
        strcpy(xyz,arg[iarg+1]);
      } else error->all(FLERR,"Illegal processors command");
      iarg += 2;

    } else if (strcmp(arg[iarg],"part") == 0) {
      if (iarg+4 > narg) error->all(FLERR,"Illegal processors command");
      if (universe->nworlds == 1)
        error->all(FLERR,
                   "Cannot use processors part command "
                   "without using partitions");
      int isend = force->inumeric(FLERR,arg[iarg+1]);
      int irecv = force->inumeric(FLERR,arg[iarg+2]);
      if (isend < 1 || isend > universe->nworlds ||
          irecv < 1 || irecv > universe->nworlds || isend == irecv)
        error->all(FLERR,"Invalid partitions in processors part command");
      if (isend-1 == universe->iworld) {
        if (send_to_partition >= 0)
          error->all(FLERR,
                     "Sending partition in processors part command "
                     "is already a sender");
        send_to_partition = irecv-1;
      }
      if (irecv-1 == universe->iworld) {
        if (recv_from_partition >= 0)
          error->all(FLERR,
                     "Receiving partition in processors part command "
                     "is already a receiver");
        recv_from_partition = isend-1;
      }

      // only receiver has otherflag dependency

      if (strcmp(arg[iarg+3],"multiple") == 0) {
        if (universe->iworld == irecv-1) {
          otherflag = 1;
          other_style = MULTIPLE;
        }
      } else error->all(FLERR,"Illegal processors command");
      iarg += 4;

    } else if (strcmp(arg[iarg],"file") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal processors command");
      delete [] outfile;
      int n = strlen(arg[iarg+1]) + 1;
      outfile = new char[n];
      strcpy(outfile,arg[iarg+1]);
      iarg += 2;

    } else if (strcmp(arg[iarg],"user") == 0) {
      if (iarg+2 > narg) error->all(FLERR,"Illegal processors command");
      gridflag = USER;
      user_part = 1;
      delete [] customfile;
      int n = strlen(arg[iarg+1]) + 1;
      customfile = new char[n];
      strcpy(customfile,arg[iarg+1]);
      iarg += 2;

    } else error->all(FLERR,"Illegal processors command");
  }

  // error checks

  if (gridflag == NUMA && mapflag != CART)
    error->all(FLERR,"Processors grid numa and map style are incompatible");
  if (otherflag && (gridflag == NUMA || gridflag == CUSTOM))
    error->all(FLERR,
               "Processors part option and grid style are incompatible");
}

/* ----------------------------------------------------------------------
   return # of bytes of allocated memory
------------------------------------------------------------------------- */

bigint Comm::memory_usage()
{
  bigint bytes = 0;
  bytes += nprocs * sizeof(int);    // grid2proc
  for (int i = 0; i < nswap; i++)
    bytes += memory->usage(sendlist[i],maxsendlist[i]);
  bytes += memory->usage(buf_send,maxsend+bufextra);
  bytes += memory->usage(buf_recv,maxrecv);
  return bytes;
}

/* ----------------------------------------------------------------------
   lees-edwards fix
------------------------------------------------------------------------- */

void Comm::lees_edwards(int n)
{
  int i,j;
  tagint idim,otherdims;
  double ss;
  double **x = atom->x;
  double **v = atom->v;
  tagint *image = atom->image;
  int nlocal = atom->nlocal;

  if (n && le_nmax < le_nall - nlocal){
    le_nmax = le_nall - nlocal;
    le_sh = (int *) memory->srealloc(le_sh,le_nmax*sizeof(int),"comm:le_sh");
  }

  if (fabs(domain->boxhi[0] - domain->subhi[0]) < EPS){
    ss = 0.5*(domain->subhi[0]+domain->sublo[0]);
    for(i=nlocal; i<le_nall; i++)
      if (x[i][0] > ss){
        x[i][1] += shift;
        v[i][1] += u_le;
        j = i-nlocal;
        if (n){
          le_sh[j] = 0;
          while (x[i][1] >= domain->boxhi[1]){
            x[i][1] -= domain->yprd;
            idim = (image[i] >> IMGBITS) & IMGMASK;
            otherdims = image[i] ^ (idim << IMGBITS);
            idim++;
            idim &= IMGMASK;
            image[i] = otherdims | (idim << IMGBITS);
            le_sh[j]--;
          }
          while (x[i][1] < domain->boxlo[1]){
            x[i][1] += domain->yprd;
            idim = (image[i] >> IMGBITS) & IMGMASK;
            otherdims = image[i] ^ (idim << IMGBITS);
            idim--;
            idim &= IMGMASK;
            image[i] = otherdims | (idim << IMGBITS); 
            le_sh[j]++;
          }
        } else
          x[i][1] += le_sh[j]*domain->yprd;
      }
  }

  if (fabs(domain->boxlo[0] - domain->sublo[0]) < EPS){
    ss = 0.5*(domain->subhi[0]+domain->sublo[0]);
    for(i=nlocal; i<le_nall; i++)
      if (x[i][0] < ss){
        x[i][1] -= shift;
        v[i][1] -= u_le;
        j = i-nlocal;
        if (n){
          le_sh[j] = 0;
          while (x[i][1] >= domain->boxhi[1]){
            x[i][1] -= domain->yprd;
            idim = (image[i] >> IMGBITS) & IMGMASK;
            otherdims = image[i] ^ (idim << IMGBITS);
            idim++;
            idim &= IMGMASK;
            image[i] = otherdims | (idim << IMGBITS);
            le_sh[j]--; 
          }
          while (x[i][1] < domain->boxlo[1]){
            x[i][1] += domain->yprd;
            idim = (image[i] >> IMGBITS) & IMGMASK;
            otherdims = image[i] ^ (idim << IMGBITS);
            idim--;
            idim &= IMGMASK;
            image[i] = otherdims | (idim << IMGBITS);
            le_sh[j]++;
          }
        } else
          x[i][1] += le_sh[j]*domain->yprd;
      }
  }
}

/* ---------------------------------------------------------------------- 
  read the binning partition file by the zeroth processor and 
  send necessary data to all cores
---------------------------------------------------------------------- */

void Comm::read_user_file()
{
  int i, j, k, l, me_h, pr_tot, neigh_max, send_max, nn[3], ll[3];
  int **c_neigh, *nneighh, *dth;
  ifstream fr;
  string str;
  MPI_Status status;
  double *prd = domain->prd;

  bin_max = 100;
  neigh_max = 10;
  c_neigh = NULL;
  nneighh = dth = NULL;

  // read data by the zeroth core

  if (me == 0) {
    fr.open(customfile);
    if(fr.is_open()){
      fr >> pr_tot >> nbp[0] >> nbp[1] >> nbp[2];
      getline(fr,str);
      if (pr_tot != nprocs)
        error->one(FLERR,"Bad number of processors in user grid file!");
      
      memory->create(nbinh,nprocs,"comm:nbinh");
      memory->create(nneighh,nprocs,"comm:nneighh");
      memory->create(own_bin,bin_max,nprocs,"comm:own_bin");
      memory->create(c_neigh,neigh_max,nprocs,"comm:c_neigh");
      for (i = 0; i < nprocs; i++){
        fr >> me_h; 
        getline(fr,str);
        fr >> nneighh[me_h]; 
        if (nneighh[me_h] > neigh_max){
          neigh_max = nneighh[me_h];
          memory->grow(c_neigh,neigh_max,nprocs,"comm:c_neigh");
	}
        for (j = 0; j < nneighh[me_h]; j++)
	  fr >> c_neigh[j][me_h];
        getline(fr,str);
        fr >> nbinh[me_h]; 
        if (nbinh[me_h] > bin_max){
          bin_max = nbinh[me_h];
          memory->grow(own_bin,bin_max,nprocs,"comm:own_bin");
	}
        for (j = 0; j < nbinh[me_h]; j++)
	  fr >> own_bin[j][me_h];
        getline(fr,str);  
      }
      fr.close();

      send_max = -1;
      for (i = 0; i < nprocs; i++){
        k = (nneighh[i]+1)*2 + nbinh[i] + 1;
        for (j = 0; j < nneighh[i]; j++)
          k +=  nbinh[j];
        if (send_max < k)
          send_max = k;  
      }
           
    } else
      error->one(FLERR,"Cannot open user grid file");
  }

  // send necessary data to all cores

  if (nprocs > 1){
    MPI_Bcast(&nbp[0],3,MPI_INT,0,world);
    MPI_Bcast(&send_max,1,MPI_INT,0,world);
  }
  memory->create(dth,send_max,"comm:dth");
  if (me > 0)
    MPI_Recv(dth,send_max,MPI_INT,0,me,world,&status);
  if (me == 0) { 
    for (i = nprocs-1; i >= 0; i--){
      dth[0] = nneighh[i];
      dth[1] = i;
      dth[2] = nbinh[i]; 
      for (j = 0; j < nneighh[i]; j++){
        k = c_neigh[j][i];
        dth[2*j + 3] = k;
        dth[2*j + 4] = nbinh[k];     
      }
      k = (nneighh[i]+1)*2 + 1;
      for (j = 0; j < nbinh[i]; j++)
        dth[k + j] = own_bin[j][i];
      k +=  nbinh[i];
      for (j = 0; j < nneighh[i]; j++){
        me_h = c_neigh[j][i];
        for (l = 0; l < nbinh[me_h]; l++)
          dth[k + l] = own_bin[l][me_h];
        k +=  nbinh[me_h]; 
      }
      
      if (i) MPI_Send(dth,send_max,MPI_INT,i,i,world); 
    }
    memory->destroy(nbinh);
    memory->destroy(nneighh);
    memory->destroy(c_neigh);
    memory->destroy(own_bin);
    nbinh = nneighh = NULL;
    c_neigh = own_bin = NULL;
  }
  
  // unpack received data

  n_neigh = dth[0] + 1;
  memory->create(my_neigh,n_neigh,"comm:my_neigh");
  memory->create(twice_comm,n_neigh,"comm:twice_comm");
  memory->create(nbinh,n_neigh,"comm:nbinh");
  bin_max = -1; 
  for (i = 0; i < n_neigh; i++){
    my_neigh[i] = dth[2*i + 1];
    nbinh[i] = dth[2*i + 2];
    twice_comm[i] = 0;
    if (nbinh[i] > bin_max)
      bin_max = nbinh[i];
  }
  memory->create(own_bin,n_neigh,bin_max,"comm:own_bin");
  k = n_neigh*2 + 1;
  for (i = 0; i < n_neigh; i++){
    for (j = 0; j < nbinh[i]; j++)
      own_bin[i][j] = dth[k + j];
    k += nbinh[i];
  }
  memory->destroy(dth);
  dth = NULL;

  // temporary output
  ofstream fw;
  char  fname_wr[FILENAME_MAX];
  sprintf(fname_wr,"initial_out_%d.dat",me);
  fw.open(fname_wr);
  fw << nbp[0] << " " << nbp[1] << " " << nbp[2] << " \n";
  fw << n_neigh << " ";
  for (i = 0; i < n_neigh; i++)
    fw << my_neigh[i] << " ";
  fw << " \n";
  for (i = 0; i < n_neigh; i++){
    fw << my_neigh[i] << " \n";
    fw << nbinh[i] << " ";
    for (j = 0; j < nbinh[i]; j++)
      fw << own_bin[i][j] << " ";
    fw << " \n";  
  }    
  fw.close();
  // temporary output 

  // calculate min and max local bins

  for (i = 0; i < 3; i++){
    nbp_max[i] = -1;
    nbp_min[i] = 999999999;
  }
  for (i = 0; i < nbinh[0]; i++){
    bin_coords(nn,own_bin[0][i]);
    for (j = 0; j < 3; j++){
      nbp_min[j] = MIN(nbp_min[j],nn[j]);
      nbp_max[j] = MAX(nbp_max[j],nn[j]);
    }
  }

  // calculate bin basic variables

  bin_diag = 0.0;
  for (j = 0; j < 3; j++){
    bin_size[j] = prd[j]/nbp[j];
    bin_size_sq[j] = bin_size[j]*bin_size[j];
    bin_size_inv[j] = 1.0/bin_size[j];
    nbp_orig[j] = nbp[j];
    bin_diag += bin_size[j]*bin_size[j];
    box_min[j] = domain->boxlo[j];
    nbp_loc[j] = nbp_max[j] - nbp_min[j] + 1; 
  }
  bin_diag = sqrt(bin_diag);

  // temporary output 
  //printf("READ: me = %d: bin_size: %f %f %f; nbp_orig: %d %d %d; box_min: %f %f %f; nbp_min: %d %d %d; nbp_max: %d %d %d; bin_diag: %f \n",me,bin_size[0],bin_size[1],bin_size[2],nbp_orig[0],nbp_orig[1],nbp_orig[2],box_min[0],box_min[1],box_min[2],nbp_min[0],nbp_min[1],nbp_min[2],nbp_max[0],nbp_max[1],nbp_max[2],bin_diag);
  // temporary output 

  // create preliminary bit matrix for local particles

  memory->create(m_comm,nbp_loc[0],nbp_loc[1],nbp_loc[2],"comm:m_comm");

  for (i = 0; i < nbp_loc[0]; i++)
    for (j = 0; j < nbp_loc[1]; j++)
      for (k = 0; k < nbp_loc[2]; k++)
        m_comm[i][j][k] = 0;

  local_bit = 1 << 0;
    
  for (j = 0; j < nbinh[0]; j++){
    bin_coords_local(nn,own_bin[0][j]);
    m_comm[nn[0]][nn[1]][nn[2]] |= local_bit;
  }
  
  // temporary output 
  sprintf(fname_wr,"initial_matrix_%d.plt",me);
  fw.open(fname_wr);
  fw << "VARIABLES=\"x\",\"y\",\"z\",\"ind\" \n";
  fw << "ZONE I=" << nbp_loc[0] << ", J=" << nbp_loc[1] << ", K=" << nbp_loc[2] << ", F=POINT \n";
  for (k = 0; k < nbp_loc[2]; k++)
    for (j = 0; j < nbp_loc[1]; j++)
      for (i = 0; i < nbp_loc[0]; i++){
        fw << i+nbp_min[0] << " " << j+nbp_min[1]  << " " << k+nbp_min[2]  << " "; 
        if (m_comm[i][j][k] & local_bit)
          fw << "1 \n"; 
        else 
          fw << "0 \n";
      }
  fw.close();
  // temporary output 
}

/* ----------------------------------------------------------------------
  process the bin data: 
  1) check whether some core neighbors can be deleted
  2) gather all updated neighbor lists to the zeroth core and 
     reorder them for a proper communication pattern 
  3) send ordered lists back to all cores, 
     reorder the bin arrays accordingly, and 
     extend the global binning
---------------------------------------------------------------------- */

void Comm::process_bin_data()
{
  int i, j, k, l, m, kb, kn, neigh_max, ind, ind1, ind2, n_min, n_max, k_max, ll[3], nn[3], pr[3];
  int *nneighh, *dth, *dth1, *p_ind;
  int *displs, *rcounts;
  double rr;

  nneighh = dth = dth1 = p_ind = displs = rcounts = NULL;

  for (j = 0; j < 3; j++){
    period_own[j] = 0;
    bin_ext[j] = static_cast<int>(cutghost[j]*bin_size_inv[j]) + 1;
    box_min[j] -= bin_ext[j]*bin_size[j];
  }

  // temporary output 
  //printf("PROCESSED: me = %d: bin_ext: %d %d %d; box_min: %f %f %f \n",me,bin_ext[0],bin_ext[1],bin_ext[2],box_min[0],box_min[1],box_min[2]);
  // temporary output

  // check the neighbors, first myself for periodic BCs, then all others

  nn[0] = nn[1] = nn[2] = 0;
  for (i = 0; i < 3; i++)
    if (periodicity[i])
      for (j = 0; j < nbinh[0]; j++){
        for (k = j+1; k < nbinh[0]; k++){
          bin_dist_periodic(ll,own_bin[0][j],own_bin[0][k],i);
          if (((ll[0]-1)*bin_size[0] < cutghost[0]) && ((ll[1]-1)*bin_size[1] < cutghost[1]) && ((ll[2]-1)*bin_size[2] < cutghost[2])){
            rr = sqrt(ll[0]*ll[0]*bin_size_sq[0] + ll[1]*ll[1]*bin_size_sq[1] + ll[2]*ll[2]*bin_size_sq[2]);
            if (rr < bin_diag + cutghost[0]){
              nn[i] = 2;
              period_own[i] = 1;
              break;
	    }
	  }
        }
        if (nn[i]) break; 
      }
  n_own = nn[0] + nn[1] + nn[2]; 
 
  if (n_own == 0)
    my_neigh[0] = -1;
  
  for (i = 1; i < n_neigh; i++){
    ind = ind1 = 0;
    for (j = 0; j < nbinh[0]; j++)
      for (k = 0; k < nbinh[i]; k++){
        bin_dist(ll,own_bin[0][j],own_bin[i][k],pr);
        if (((ll[0]-1)*bin_size[0] < cutghost[0]) && ((ll[1]-1)*bin_size[1] < cutghost[1]) && ((ll[2]-1)*bin_size[2] < cutghost[2])){
          rr = sqrt(ll[0]*ll[0]*bin_size_sq[0] + ll[1]*ll[1]*bin_size_sq[1] + ll[2]*ll[2]*bin_size_sq[2]);
          if (rr < bin_diag + cutghost[0]){
            if (pr[0] || pr[1] || pr[2]){
              for (m = 0; m < 3; m++)
                if (pr[m] && period_own[m] == 0) 
                  ind1 = 1; 
            } else 
              ind = 1;
	  }
	}
      }
  
    if (ind == 0 && ind1 == 0){
      my_neigh[i] = -1;
    } else{
      if (ind == 1 && ind1 == 1)
        twice_comm[i] = 1;
    }
  }

  neigh_max = n_neigh; 
  memory->create(dth1,2*neigh_max,"comm:dth1");
  n_neigh = 0;
  for (i = 1; i <neigh_max; i++)
    if (my_neigh[i] > -1){
      dth1[n_neigh] = my_neigh[i];
      n_neigh++; 
      if (twice_comm[i]){
        dth1[n_neigh] = my_neigh[i];
        n_neigh++; 
      }
    }

  /*printf("00 me=%d neigh: %d - ",me,n_neigh);
  for (m = 0; m < n_neigh; m++)
    printf("%d ",dth1[m]);
  printf("\n");
  sleep(2);*/

  // temporary output 
  //printf("PROCESSED: me = %d: n_own: %d; n_neigh: %d; neigh_max: %d \n",me,n_own,n_neigh,neigh_max);
  // temporary output  

  // gather all updated neighbor lists to the zeroth core
  
  if (me == 0){
    memory->create(rcounts,nprocs,"comm:rcounts");
    memory->create(displs,nprocs,"comm:displs");
    memory->create(nneighh,nprocs,"comm:nneighh");
    memory->create(p_ind,nprocs,"comm:p_ind");
  }

  if (nprocs > 1)
    MPI_Gather(&n_neigh, 1, MPI_INT, rcounts, 1, MPI_INT, 0, world);
  
  int cc[nprocs];
  if (me == 0){
    n_max = 0;
    n_min = 999999; 
    kn = 0;
    for (i = 0; i < nprocs; i++){
      p_ind[i] = -1;
      cc[i] = 0;
      displs[i] = kn;
      kn += rcounts[i];
      n_max = MAX(n_max,rcounts[i]);
      n_min = MIN(n_min,rcounts[i]); 
    }
    memory->create(dth,kn,"comm:dth");
    l = 0;
    for (i = n_max; i >= n_min; i--)
      for (j = 0; j < nprocs; j++)
        if (rcounts[j] == i){
          nneighh[l] = j;
          l++;  
        }

    if (l != nprocs)
      error->one(FLERR,"Something went wrong in sorting cores according to the number of neighbors!");
  }
  if (nprocs > 1)
    MPI_Gatherv(dth1, n_neigh, MPI_INT, dth, rcounts, displs, MPI_INT, 0, world);

  // reorder neighbors for a proper communication pattern 

  if (me == 0){
    ind = 1;
    kb = 0;
    while (ind){
      for (j = 0; j < nprocs; j++){
        l = nneighh[j];
        if (cc[l] == rcounts[l])  
          p_ind[l] = kb;
        if (p_ind[l] < kb){ 
          for (m = displs[l] + cc[l]; m < displs[l] + rcounts[l]; m++){
            ind1 = ind2 = 0;
            k = dth[m]; 
            if (p_ind[k] < kb) ind1 = 1;
            if (ind1)
              for (i = displs[k] + cc[k]; i < displs[k] + rcounts[k]; i++)
                if (dth[i] == l){
                  ind2 = 1;
                  break; 
		}
            if (ind2){
              k_max = dth[m]; 
              dth[m] = dth[displs[l] + cc[l]];
              dth[displs[l] + cc[l]] = k_max;
              p_ind[l] = kb;
              cc[l]++;
              k_max = dth[i]; 
              dth[i] = dth[displs[k] + cc[k]];
              dth[displs[k] + cc[k]] = k_max;
              p_ind[k] = kb;
              cc[k]++;
              break; 
            }
	  }
          if (!ind2)
            p_ind[l] = kb;   
	}
      }
      kb++;
      if (kb >= n_max){
        k_max = 0;
        for (j = 0; j < nprocs; j++)
          k_max += cc[j];
	if (k_max == kn) ind = 0;  
      }
    }
  }

  // send ordered lists back to all cores and reorder the bin arrays accordingly

  if (nprocs > 1)
    MPI_Scatterv(dth, rcounts, displs, MPI_INT, dth1, n_neigh, MPI_INT, 0, world);

  if (me == 0){
    memory->destroy(dth);
    memory->destroy(rcounts);
    memory->destroy(displs);
    memory->destroy(nneighh);
    memory->destroy(p_ind);
    dth = rcounts = displs = nneighh = p_ind = NULL;
  }

  /*printf("me=%d here 00 %d %d %d \n",me,n_neigh,neigh_max,bin_max);
  sleep(6);

  printf("me=%d neigh: %d - ",me,n_neigh);
  for (m = 0; m < n_neigh; m++)
    printf("%d ",dth1[m]);
  printf("\n");
  sleep(2);

  printf("me=%d my_neigh: %d - ",me,neigh_max);
  for (m = 0; m < neigh_max; m++)
    printf("%d ",my_neigh[m]);
  printf("\n");
  sleep(2);

  printf("me=%d twice_comm: %d - ",me,neigh_max);
  for (m = 0; m < neigh_max; m++)
    printf("%d ",twice_comm[m]);
  printf("\n");
  sleep(2);*/

  int c_ind[2*neigh_max];
  memory->create(p_ind,bin_max,"comm:p_ind");
  memory->create(neigh_ind,n_neigh,"comm:neigh_ind"); 
  j = 1;
  k_max = 0;
  for (i = 0; i < n_neigh; i++){
    ind = 1;
    for (m = 0; m < k_max; m++)
      if (dth1[i] == c_ind[m]){
        ind = 0;
        break;  
      }
    //printf("me=%d i=%d %d %d \n",me,i,j,ind);
    //sleep(2);

    if (ind){
      k = j;
      while (k < neigh_max){
        if (dth1[i] == my_neigh[k])
          break;
        k++;  
      }
      if (k >= neigh_max)
        error->one(FLERR,"The neighbor has not been found in my_neigh array!");
      neigh_ind[i] = j;
      if (k > j){
        //printf("me=%d k=%d, %d %d, nj=%d, nk=%d \n",me,k,my_neigh[j],my_neigh[k],nbinh[j],nbinh[k]); 
        //sleep(2); 
        if (my_neigh[j] == -1){
          my_neigh[j] = my_neigh[k]; 
          nbinh[j] = nbinh[k];
          twice_comm[j] = twice_comm[k];
          for (m = 0; m < nbinh[k]; m++)
            own_bin[j][m] = own_bin[k][m];
          my_neigh[k] = -1;
          twice_comm[k] = 0; 
        } else{
          for (m = 0; m < nbinh[j]; m++)
            p_ind[m] = own_bin[j][m];
          for (m = 0; m < nbinh[k]; m++)
            own_bin[j][m] = own_bin[k][m];
          for (m = 0; m < nbinh[j]; m++)
            own_bin[k][m] = p_ind[m];
          m = my_neigh[j];
          my_neigh[j] = my_neigh[k];
          my_neigh[k] = m;
          m = twice_comm[j];
          twice_comm[j] = twice_comm[k];
          twice_comm[k] = m;
          m = nbinh[j]; 
          nbinh[j] = nbinh[k]; 
          nbinh[k] = m;
        }
      }
      c_ind[k_max] = my_neigh[j];
      k_max++;
      j++;  
    } else{
      ind1 = 1;
      for (m = 1; m <= j; m++)
        if (dth1[i] == my_neigh[m]){
          neigh_ind[i] = m;
          ind1 = 0;
          break;
	}
      if (ind1)
        error->one(FLERR,"The twice neighbor has not been found in my_neigh array!");
    }
  }
  neigh_max = j;
  memory->destroy(p_ind);
  memory->destroy(dth1);
  p_ind = dth1 = NULL;

  //printf("me=%d here 0 \n",me);
  //sleep(6);

  // extend the global binning

  for (j = 0; j < 3; j++){
    ll[j] = nbp[j];
    ll[j] += 2*bin_ext[j]; 
  }

  for (i = 0; i < neigh_max; i++)
    for (j = 0; j < nbinh[i]; j++){ 
      bin_coords(nn,own_bin[i][j]);
      for (k = 0; k < 3; k++) 
        nn[k] += bin_ext[k]; 
      own_bin[i][j] = nn[2]*ll[1]*ll[0] + nn[1]*ll[0] + nn[0];
    }

  for (j = 0; j < 3; j++){
    nbp[j] = ll[j];
    nbp_max[j] += 2*bin_ext[j];
  }

  //printf("me=%d here 1 \n",me);
  //sleep(3);

  // temporary output
  ofstream fw;
  char  fname_wr[FILENAME_MAX];
  sprintf(fname_wr,"processed_data_%d.dat",me);
  fw.open(fname_wr);
  fw << nbp[0] << " " << nbp[1] << " " << nbp[2] << " \n";
  fw << n_neigh + 1 << " ";
  fw << my_neigh[0] << " ";
  for (i = 0; i < n_neigh; i++)
    fw << my_neigh[neigh_ind[i]] << " ";
  fw << " \n";
  fw << my_neigh[0] << " \n";
  fw << nbinh[0] << " ";
  for (j = 0; j < nbinh[0]; j++)
    fw << own_bin[0][j] << " ";
  fw << " \n"; 
  for (i = 0; i < n_neigh; i++){
    fw << my_neigh[neigh_ind[i]] << " \n";
    fw << nbinh[neigh_ind[i]] << " ";
    for (j = 0; j < nbinh[neigh_ind[i]]; j++)
      fw << own_bin[neigh_ind[i]][j] << " ";
    fw << " \n";  
  }    
  fw.close();
  // temporary output 

  //printf("me=%d here 2 \n",me);
  //sleep(3);  

}

/* ---------------------------------------------------------------------- 
  build communication matrix which marks various bins: to send, to receive, to exchange
---------------------------------------------------------------------- */

void Comm::build_comm_matrix()
{
  int i, j, k, l, m, ii[3], ll[3], nn[3], pr[3], nl_min[3], nl_max[3], k1, k2, ind, ind1, ind2, k_max;
  bigint inv_mask;
  double rr;

  for (j = 0; j < 3; j++)
    nbp_loc[j] = nbp_max[j] - nbp_min[j] + 1;

  memory->destroy(m_comm);
  m_comm = NULL;
  memory->create(m_comm,nbp_loc[0],nbp_loc[1],nbp_loc[2],"comm:m_comm");

  for (i = 0; i < nbp_loc[0]; i++)
    for (j = 0; j < nbp_loc[1]; j++)
      for (k = 0; k < nbp_loc[2]; k++)
        m_comm[i][j][k] = 0;

  // create bit masks

  nswap = n_neigh + n_own;
  grow_swap(nswap); 
  if (2*n_neigh + n_own + 1 > MAX_GBITS)
    error->one(FLERR,"Not enough bits for the communication matrix!");
  
  memory->create(send_bit,n_neigh,"comm:send_bit");
  memory->create(exchange_bit,n_neigh,"comm:exchange_bit");
  
  for (i = 0; i < n_neigh; i++){
    send_bit[i] = (bigint)1 << i;
    k = n_neigh + i;
    exchange_bit[i] = (bigint)1 << k;
  }
  k = 2*n_neigh;
  local_bit = (bigint)1 << k;
  k++;
  for (i = 0; i < n_own; i++){
    j = k + i; 
    own_bit[i] = (bigint)1 << j;
  }
  /*printf("BITS_TEST: me = %d; test_bit = %ld \n",me,(bigint)1 << 39);

  printf("BITS: me = %d; send_bit = ",me);
  for (i = 0; i < n_neigh; i++)
    printf("%ld ",send_bit[i]);
  printf("; exchange_bit = ");
  for (i = 0; i < n_neigh; i++)
    printf("%ld ",exchange_bit[i]);
  printf("; local_bit = %ld; ",local_bit);
  printf("own_bit = ");
  for (i = 0; i < n_own; i++)
    printf("%ld ",own_bit[i]);
    printf("\n");*/

  // assign local atom bits

  for (j = 0; j < nbinh[0]; j++){
    bin_coords_local(nn,own_bin[0][j]);
    m_comm[nn[0]][nn[1]][nn[2]] |= local_bit;
  }

  //printf("me=%d here 1 \n",me);
  //sleep(3); 

  // assign bits for sending atoms within the same core due to periodic BCs 

  if (my_neigh[0] > -1){
    l = 0; 
    for (i = 0; i < 3; i++)
      if (periodicity[i]){
        if (l == n_own)
          break;
        ind = 0; 
        k1 = n_neigh + l;
        k2 = k1 + 1; 
        pbc_flag[k1] = 0; 
        pbc_flag[k2] = 0;
        sendproc[k1] = recvproc[k1] = me;
        sendproc[k2] = recvproc[k2] = me;
        for (j = 0; j < 6; j++){
          pbc[k1][j] = 0;  
          pbc[k2][j] = 0;
        }
          
        for (j = 0; j < nbinh[0]; j++){
          bin_coords(nn,own_bin[0][j]);
          if (nn[i] > nbp_max[i] - 2*bin_ext[i]){

            for (ii[0] = nbp_min[0]; ii[0] <= nbp_max[0]; ii[0]++)
              for (ii[1] = nbp_min[1]; ii[1] <= nbp_max[1]; ii[1]++)
                for (ii[2] = nbp_min[2]; ii[2] <= nbp_max[2]; ii[2]++)
                  if (ii[i] >= nbp_min[i] + bin_ext[i] && ii[i] < nbp_min[i] + 2*bin_ext[i]){
                    k = ii[2]*nbp[0]*nbp[1] + ii[1]*nbp[0] + ii[0];
                    bin_dist_periodic(ll,own_bin[0][j],k,i);
                    if (((ll[0]-1)*bin_size[0] < cutghost[0]) && ((ll[1]-1)*bin_size[1] < cutghost[1]) && ((ll[2]-1)*bin_size[2] < cutghost[2])){
                      rr = sqrt(ll[0]*ll[0]*bin_size_sq[0] + ll[1]*ll[1]*bin_size_sq[1] + ll[2]*ll[2]*bin_size_sq[2]);
                      if (rr < bin_diag + cutghost[0]){
                        /*for (m = 0; m < 3; m++)
                          if (ii[m] - nbp_min[m] < 0 || ii[m] - nbp_min[m] >= nbp_loc[m]) {
                            printf("me=%d: %d %d %d %d \n",me,ii[m],nbp_min[m],nbp_max[m],nbp_loc[m]);
                            sleep(2);
			  }
                        if (l >= n_own){
                          printf("0 me=%d: %d %d \n",me,l,n_own);
                          sleep(2);
			  }*/
                        m_comm[ii[0]-nbp_min[0]][ii[1]-nbp_min[1]][ii[2]-nbp_min[2]] |= own_bit[l]; 
                        pbc_flag[k1] = 1;
                        pbc[k1][i] = 1;
                        ind = 1;
                      }
                    }
                  }
 
          } else if (nn[i] < nbp_min[i] + 2*bin_ext[i]) {

            for (ii[0] = nbp_min[0]; ii[0] <= nbp_max[0]; ii[0]++)
              for (ii[1] = nbp_min[1]; ii[1] <= nbp_max[1]; ii[1]++)
                for (ii[2] = nbp_min[2]; ii[2] <= nbp_max[2]; ii[2]++)
                  if (ii[i] > nbp_max[i] - 2*bin_ext[i] && ii[i] <= nbp_max[i] - bin_ext[i]){
                    k = ii[2]*nbp[0]*nbp[1] + ii[1]*nbp[0] + ii[0];
                    bin_dist_periodic(ll,own_bin[0][j],k,i);
                    if (((ll[0]-1)*bin_size[0] < cutghost[0]) && ((ll[1]-1)*bin_size[1] < cutghost[1]) && ((ll[2]-1)*bin_size[2] < cutghost[2])){
                      rr = sqrt(ll[0]*ll[0]*bin_size_sq[0] + ll[1]*ll[1]*bin_size_sq[1] + ll[2]*ll[2]*bin_size_sq[2]);
                      if (rr < bin_diag + cutghost[0]){
                        /*for (m = 0; m < 3; m++)
                          if (ii[m] - nbp_min[m] < 0 || ii[m] - nbp_min[m] >= nbp_loc[m]) { 
                            printf("me=%d: %d %d %d %d \n",me,ii[m],nbp_min[m],nbp_max[m],nbp_loc[m]);
                            sleep(2);
			  }
                        if (l+1 >= n_own){
                          printf("1 me=%d: %d %d \n",me,l+1,n_own);
                          sleep(2);
			  }*/
                        m_comm[ii[0]-nbp_min[0]][ii[1]-nbp_min[1]][ii[2]-nbp_min[2]] |= own_bit[l+1]; 
                        pbc_flag[k2] = 1;
                        pbc[k2][i] = -1;
                        ind = 1;
                      }
                    }
                  } 

          }  
        }  
        if (ind) l += 2;
      }
    if (l != n_own) 
      error->one(FLERR,"Something went wrong in marking own bits for the communication matrix!");

    ind2 = 0;
    for (i = 0; i < n_own; i++){ 
      l = n_neigh + i;
      inv_mask = own_bit[i] ^ ~(bigint)0;
      for (j = 0; j < nbinh[0]; j++){
        bin_coords_local(nn,own_bin[0][j]);
        bin_coords(ll,own_bin[0][j]);
        if (m_comm[nn[0]][nn[1]][nn[2]] & own_bit[i]){
          ind = 0;
          for (m = 0; m < 3; m++){
            ll[m] += pbc[l][m]*nbp_orig[m];
            if (ll[m] < nbp_min[m] || ll[m] > nbp_max[m]){
              ind = 1;
              break;
            }
          }
          if (ind){
            m_comm[nn[0]][nn[1]][nn[2]]  &= inv_mask;
            ind2 = 1;
          } 
        }
      }
    }
    if (ind2) printf("CORRECTION OF MY_OWN COMMUNICATION MATRIX: proc = %d! \n",me);
  }

  //printf("me=%d here 2 \n",me);
  //sleep(3); 

  // assign bits for the communication with neighbors

  int c_ind[n_neigh+1],cc[4],kk;
  int per_check = 0;
  ind2 = 0;
  k_max = 0;
  for (i = 0; i < n_neigh; i++){
    for (m = 0; m < 4; m++)
      cc[m] = 0; 
    l = neigh_ind[i];
    pbc_flag[i] = 0;
    for (j = 0; j < 6; j++)
      pbc[i][j] = 0;
    sendproc[i] = my_neigh[l];
    recvproc[i] = my_neigh[l];
    
    ind = 0;
    for (m = 0; m < k_max; m++)
      if (my_neigh[l] == c_ind[m]){
        ind = 1;
        break;  
      }  
    if (twice_comm[l] && ind == 0){
      c_ind[k_max] = my_neigh[l];
      k_max++; 
    }

    for (m = 0; m < 3; m++){
      nl_max[m] = -1;
      nl_min[m] = 999999999;
    }
    for (m = 0; m < nbinh[l]; m++){
      bin_coords(nn,own_bin[l][m]);
      for (j = 0; j < 3; j++){
        nl_min[j] = MIN(nl_min[j],nn[j]);
        nl_max[j] = MAX(nl_max[j],nn[j]);
      }
    }
    for (m = 0; m < 3; m++){
      nl_min[m] -= bin_ext[m]; 
      nl_max[m] += bin_ext[m];
      //if (nl_min[m] < 0 || nl_max[m] >= nbp[m]) 
      //  printf("PROBLEM HERE: me=%d; to=%d; m=%d; nl: %d %d \n",me,sendproc[i],m,nl_min[m],nl_max[m]);
    }  

    for (j = 0; j < nbinh[0]; j++)
      for (k = 0; k < nbinh[l]; k++){
        bin_dist(ll,own_bin[0][j],own_bin[l][k],pr);
        if (((ll[0]-1)*bin_size[0] < cutghost[0]) && ((ll[1]-1)*bin_size[1] < cutghost[1]) && ((ll[2]-1)*bin_size[2] < cutghost[2])){
          rr = sqrt(ll[0]*ll[0]*bin_size_sq[0] + ll[1]*ll[1]*bin_size_sq[1] + ll[2]*ll[2]*bin_size_sq[2]);
          if (rr < bin_diag + cutghost[0]){
            bin_coords_local(nn,own_bin[0][j]);
            bin_coords_local(ll,own_bin[l][k]);
            if (twice_comm[l]){
              if (ind){ 
                if (pr[0] || pr[1] || pr[2]){
                  ind1 = 0;
                  for (m = 0; m < 3; m++)
                    if (pr[m] && period_own[m] == 0)
                      ind1 = 1;
                  if (ind1){
                    kk = 0; 
                    m_comm[nn[0]][nn[1]][nn[2]] |= send_bit[i];
                    for (m = 0; m < 3; m++)
                      if (pr[m] && period_own[m] == 0){
                        kk++; 
                        pbc_flag[i] = 1;
                        if (ll[m] < nn[m]){
                          pbc[i][m] = -1;
                          ll[m] += nbp_orig[m]; 
                        } else {
                          pbc[i][m] = 1;
                          ll[m] -= nbp_orig[m]; 
                        } 
		      }
                    m_comm[ll[0]][ll[1]][ll[2]] |= exchange_bit[i];
                    cc[kk] = 1;
		  }
                }
	      } else{
                if (pr[0] == 0 && pr[1] == 0 && pr[2] == 0){
                  m_comm[nn[0]][nn[1]][nn[2]] |= send_bit[i]; 
                  m_comm[ll[0]][ll[1]][ll[2]] |= exchange_bit[i];  
		}
	      }
            } else{
              m_comm[nn[0]][nn[1]][nn[2]] |= send_bit[i];
              kk = 0;
              for (m = 0; m < 3; m++)
                if (pr[m] && period_own[m] == 0){
                  kk++; 
                  pbc_flag[i] = 1;
                  if (ll[m] < nn[m]){
                    pbc[i][m] = -1;
                    ll[m] += nbp_orig[m]; 
                  } else {
                    pbc[i][m] = 1;
                    ll[m] -= nbp_orig[m]; 
                  } 
                }
              m_comm[ll[0]][ll[1]][ll[2]] |= exchange_bit[i];
              cc[kk] = 1; 
	    } 
	  }
	}
      }

    inv_mask = send_bit[i] ^ ~(bigint)0;
    for (j = 0; j < nbinh[0]; j++){
      bin_coords_local(nn,own_bin[0][j]); 
      bin_coords(ll,own_bin[0][j]); 
      if (m_comm[nn[0]][nn[1]][nn[2]] & send_bit[i]){
        ind = 0;
        for (m = 0; m < 3; m++){
          ll[m] += pbc[i][m]*nbp_orig[m];
          if (ll[m] < nl_min[m] || ll[m] > nl_max[m]){
            ind = 1;
            break;  
	  }
        }
        if (ind){
          m_comm[nn[0]][nn[1]][nn[2]]  &= inv_mask;  
          ind2 = 1;
        }
      }
    }

    kk = 0;
    for (m = 0; m < 4; m++)
      kk += cc[m];
    if (kk > 1) {
      per_check = 1;
      //printf("per_check me=%d, i=%d, pbc=%d %d %d %d \n",me,i,pbc_flag[i],pbc[i][0],pbc[i][1],pbc[i][2]);
    }  
  }

  if (ind2) printf("CORRECTION OF NEIGHBOR COMMUNICATION MATRIX: proc = %d! \n",me);

  //printf("me=%d here 3 \n",me);
  //sleep(3); 

  if (per_check) error->warning(FLERR,"The simulation may not function properly due to complicated periodic BCs!!!");   

  // temporary output 
  //printf("PERIODIC: me = %d: nneigh=%d; ",me,n_neigh);
  //for (l = 0; l < n_neigh; l++)
  //  printf("to=%d pbc: %d - %d %d %d; ",sendproc[l],pbc_flag[l],pbc[l][0],pbc[l][1],pbc[l][2]);
  //printf(" \n");

  //printf("BUILD: me = %d: cutghost: %f %f %f; nbp_min: %d %d %d; nbp_max: %d %d %d \n",me,cutghost[0],cutghost[1],cutghost[2],nbp_min[0],nbp_min[1],nbp_min[2],nbp_max[0],nbp_max[1],nbp_max[2]);
  ofstream fw;
  char  fname_wr[FILENAME_MAX];
  sprintf(fname_wr,"local_matrix_%d.plt",me);
  fw.open(fname_wr);
  fw << "VARIABLES=\"x\",\"y\",\"z\",\"ind\" \n";
  fw << "ZONE I=" << nbp_loc[0] << ", J=" << nbp_loc[1] << ", K=" << nbp_loc[2] << ", F=POINT \n";
  for (k = 0; k < nbp_loc[2]; k++)
    for (j = 0; j < nbp_loc[1]; j++)
      for (i = 0; i < nbp_loc[0]; i++){
        fw << i+nbp_min[0] << " " << j+nbp_min[1]  << " " << k+nbp_min[2]  << " "; 
        if (m_comm[i][j][k] & local_bit)
          fw << "1 \n"; 
        else 
          fw << "0 \n";
      }
  fw.close();

  if (n_neigh > 0){
    sprintf(fname_wr,"neigh_matrix_%d.plt",me);
    fw.open(fname_wr);
    fw << "VARIABLES=\"x\",\"y\",\"z\"";
    for (l = 0; l < n_neigh; l++)
      fw << ",\"s" << l << "\"";
    for (l = 0; l < n_neigh; l++)
      fw << ",\"e" << l << "\"";
    fw << " \n";
    fw << "ZONE I=" << nbp_loc[0] << ", J=" << nbp_loc[1] << ", K=" << nbp_loc[2] << ", F=POINT \n";
    for (k = 0; k < nbp_loc[2]; k++)
      for (j = 0; j < nbp_loc[1]; j++)
        for (i = 0; i < nbp_loc[0]; i++){
          fw << i+nbp_min[0] << " " << j+nbp_min[1]  << " " << k+nbp_min[2]  << " ";
          for (l = 0; l < n_neigh; l++){
            if (m_comm[i][j][k] & send_bit[l])
              fw << "1 "; 
            else 
              fw << "0 ";
	  }
          for (l = 0; l < n_neigh; l++){
            if (m_comm[i][j][k] & exchange_bit[l])
              fw << "1 "; 
            else 
              fw << "0 ";
	  }
          fw << " \n";
        }
    fw.close();
  }
 
  if (n_own > 0){
    sprintf(fname_wr,"own_matrix_%d.plt",me);
    fw.open(fname_wr);
    fw << "VARIABLES=\"x\",\"y\",\"z\"";
    for (l = 0; l < n_own; l++)
      fw << ",\"o" << l << "\"";
    fw << " \n";
    fw << "ZONE I=" << nbp_loc[0] << ", J=" << nbp_loc[1] << ", K=" << nbp_loc[2] << ", F=POINT \n";
    for (k = 0; k < nbp_loc[2]; k++)
      for (j = 0; j < nbp_loc[1]; j++)
        for (i = 0; i < nbp_loc[0]; i++){
          fw << i+nbp_min[0] << " " << j+nbp_min[1]  << " " << k+nbp_min[2]  << " "; 
          for (l = 0; l < n_own; l++){
            if (m_comm[i][j][k] & own_bit[l])
              fw << "1 "; 
            else 
              fw << "0 ";
	  }
          fw << " \n";
        }
    fw.close();
  } 
  // temporary output

  processed_data_ind = 1;
}

/* ---------------------------------------------------------------------- 
  find the distance (ll[0],ll[1],ll[2]) between bins b1 and b2
---------------------------------------------------------------------- */

void Comm::bin_dist(int *ll, int &b1, int &b2, int *pr)
{
  int i, n1[3], n2[3];

  bin_coords(n1,b1);
  bin_coords(n2,b2);

  for (i = 0; i < 3; i++){
    pr[i] = 0;
    ll[i] = abs(n1[i]-n2[i]);
    if (periodicity[i] && ll[i] > 0.5*nbp_orig[i]){
      ll[i] = nbp_orig[i] - ll[i];
      pr[i] = 1;
    }
  }
}

/* ---------------------------------------------------------------------- 
  find the distance (ll[0],ll[1],ll[2]) between bins b1 and b2
  used only for processors which may have periodicity with themselves
---------------------------------------------------------------------- */

void Comm::bin_dist_periodic(int *ll, int &b1, int &b2, int prd_ind)
{
  int i, n1[3], n2[3], prd[3];

  for (i = 0; i < 3; i++)
    prd[i] = 0;
  prd[prd_ind] = 1;

  bin_coords(n1,b1);
  bin_coords(n2,b2);

  for (i = 0; i < 3; i++){
    ll[i] = abs(n1[i]-n2[i]);
    if (periodicity[i] && prd[i])
      ll[i] = nbp_orig[i] - ll[i];
  }
}

/* ---------------------------------------------------------------------- 
  convert the bin number bn to bin coordinates (nn[0],nn[1],nn[2])
---------------------------------------------------------------------- */

void Comm::bin_coords(int *nn, int &bn)
{
  int i;

  nn[2] = static_cast<int>(bn/nbp[0]/nbp[1]);
  i = bn - nn[2]*nbp[0]*nbp[1];
  nn[1] = static_cast<int>(i/nbp[0]);
  nn[0] = i - nn[1]*nbp[0];
}

/* ---------------------------------------------------------------------- 
  convert the bin number bn to local bin coordinates (nn[0],nn[1],nn[2])
---------------------------------------------------------------------- */

void Comm::bin_coords_local(int *nn, int &bn)
{
  int i;

  bin_coords(nn,bn);
  for (i = 0; i < 3; i++)
    nn[i] -= nbp_min[i];
}

/* ----------------------------------------------------------------------
  convert coordinates to local bin coordinates (nn[0],nn[1],nn[2])
---------------------------------------------------------------------- */

int Comm::coords_to_bin(double *xx, int *nn)
{
  int i,ind;

  ind = 1;
  for (i = 0; i < 3; i++){
    nn[i] = static_cast<int>((xx[i] - box_min[i])*bin_size_inv[i]) - nbp_min[i];
    if (nn[i] < 0 || nn[i] >= nbp_loc[i]){
      ind = 0;
      break;
    }
  }
  return ind;
}

/* ----------------------------------------------------------------------
  convert coordinates to local bin coordinates (nn[0],nn[1],nn[2]) and 
  check for periodicity
---------------------------------------------------------------------- */

int Comm::coords_to_bin_exchange(double *xx, int *nn)
{
  int i,ind;

  ind = 1;
  for (i = 0; i < 3; i++){
    nn[i] = static_cast<int>((xx[i] - box_min[i])*bin_size_inv[i]);
    if (periodicity[i]){
      if (nn[i] > nbp_max[i])
        nn[i] -= nbp_orig[i];
      if (nn[i] < nbp_min[i])    
        nn[i] += nbp_orig[i];
    }
    nn[i] -= nbp_min[i];
    if (nn[i] < 0 || nn[i] >= nbp_loc[i]){
      ind = 0;
      break;
    }
  }
  return ind;
}

/* ---------------------------------------------------------------------- */


